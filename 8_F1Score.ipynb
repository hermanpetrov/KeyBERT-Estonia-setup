{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06402b6-6f71-4642-862a-40d5ae35f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE UNIFIED CUSTOM FOLDER FOR MEASURING KEYBERT F1 ACCURACY\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_unified_data():\n",
    "    base_path = os.getcwd() \n",
    "    models_path = os.path.join(base_path, 'models')\n",
    "    unified_path = os.path.join(models_path, 'unified_data_custom')\n",
    "    os.makedirs(unified_path, exist_ok=True)\n",
    "    \n",
    "    subdirs = ['raw_text_lemma_data_LCF', 'raw_text_lemma_data', 'raw_text_data_LCF', 'raw_text_data']\n",
    "    ngrams = ['ngram_1_1', 'ngram_2_2', 'ngram_3_3']\n",
    "    diversities = [f\"diversity_{i}\" for i in range(11)]\n",
    "    models = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "    m1_path = os.path.join(base_path, 'filol_scores', 'keywords', 'philologist_M1')\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        for ngram in ngrams:\n",
    "            for diversity in diversities:\n",
    "                unified_sub_path = os.path.join(unified_path, subdir, ngram, diversity)\n",
    "                os.makedirs(unified_sub_path, exist_ok=True)\n",
    "                model_paths = [os.path.join(models_path, subdir, model, ngram, diversity) for model in models]\n",
    "                \n",
    "                example_files = os.listdir(model_paths[0])\n",
    "                for file_name in example_files:\n",
    "                    m1_file_path = os.path.join(m1_path, file_name)\n",
    "                    if os.path.exists(m1_file_path):\n",
    "                        m1_df = pd.read_csv(m1_file_path, delimiter=';', header=0)\n",
    "                        expected_words = len(m1_df) \n",
    "\n",
    "                        data_frames = {}\n",
    "                        for model, path in zip(models, model_paths):\n",
    "                            file_path = os.path.join(path, file_name)\n",
    "                            if os.path.exists(file_path):\n",
    "                                df = pd.read_csv(file_path, delimiter=';', header=0)\n",
    "                                data_frames[model] = df.iloc[:expected_words, 0]\n",
    "\n",
    "                        combined_df = pd.DataFrame(data_frames)\n",
    "                        combined_df.to_csv(os.path.join(unified_sub_path, file_name), index=False, sep=';')\n",
    "\n",
    "\n",
    "create_unified_data() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c89a5d02-f882-4e7a-8bd1-622ff9dc7759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_keywords(keywords):\n",
    "    \"\"\"Preprocess a series of keywords: lowercasing and stripping spaces.\"\"\"\n",
    "    return keywords.str.lower().str.strip()\n",
    "\n",
    "\n",
    "def calculate_f1_scores(model_base_dir, human_base_dir, output_base_dir):\n",
    "    model_scores = {}\n",
    "    count_files = 0\n",
    "\n",
    "    for diversity in range(11):\n",
    "        model_dir = os.path.join(model_base_dir, f\"diversity_{diversity}\")\n",
    "        files = [f for f in os.listdir(model_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "        for file_name in files:\n",
    "            model_file_path = os.path.join(model_dir, file_name)\n",
    "            human_file_path = os.path.join(human_base_dir, file_name)\n",
    "\n",
    "            if os.path.exists(human_file_path):\n",
    "                model_keywords = pd.read_csv(model_file_path, delimiter=\";\")\n",
    "                human_keywords = pd.read_csv(human_file_path)\n",
    "\n",
    "                human_set = set(preprocess_keywords(human_keywords[\"keyword\"].dropna()))\n",
    "                for column in model_keywords:\n",
    "                    model_set = set(\n",
    "                        preprocess_keywords(model_keywords[column].dropna())\n",
    "                    )\n",
    "                    tp = len(model_set & human_set)\n",
    "                    fp = len(model_set - human_set)\n",
    "                    fn = len(human_set - model_set)\n",
    "\n",
    "                    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "                    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "                    f1_score = (\n",
    "                        2 * precision * recall / (precision + recall)\n",
    "                        if precision + recall > 0\n",
    "                        else 0\n",
    "                    )\n",
    "\n",
    "                    if column not in model_scores:\n",
    "                        model_scores[column] = {\"total_f1\": 0, \"count\": 0}\n",
    "                    model_scores[column][\"total_f1\"] += f1_score\n",
    "                    model_scores[column][\"count\"] += 1\n",
    "\n",
    "                count_files += 1\n",
    "\n",
    "    scores_df = pd.DataFrame(\n",
    "        [{**{\"model\": model}, **data} for model, data in model_scores.items()]\n",
    "    )\n",
    "    scores_df[\"average_f1\"] = scores_df[\"total_f1\"] / scores_df[\"count\"]\n",
    "\n",
    "    output_dir = os.path.join(output_base_dir, \"f1_keybert_scores_average_results\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_file_name = model_base_dir.split(\"/\")[-2] + \".csv\"\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    scores_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    return scores_df, count_files\n",
    "\n",
    "\n",
    "configurations = [\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for model_dir, human_dir, output_base_dir in configurations:\n",
    "    scores_df, file_count = calculate_f1_scores(model_dir, human_dir, output_base_dir)\n",
    "    print(\n",
    "        f\"Processed {file_count} files for {human_dir}. Results saved to {output_base_dir} as file {model_dir.split('/')[-1]}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01024db7-d023-4c5a-8131-5e1b189e567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_keywords(keywords):\n",
    "    \"\"\"Preprocess a series of keywords: lowercasing and stripping spaces.\"\"\"\n",
    "    return keywords.str.lower().str.strip()\n",
    "\n",
    "\n",
    "def calculate_f1_scores(model_base_dir, human_base_dir, output_base_dir):\n",
    "    model_scores = {}\n",
    "    count_files = 0\n",
    "    model_diversity_scores = {}\n",
    "\n",
    "    for diversity in range(11):\n",
    "        model_dir = os.path.join(model_base_dir, f\"diversity_{diversity}\")\n",
    "        files = [f for f in os.listdir(model_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "        for file_name in files:\n",
    "            model_file_path = os.path.join(model_dir, file_name)\n",
    "            human_file_path = os.path.join(human_base_dir, file_name)\n",
    "\n",
    "            if os.path.exists(human_file_path):\n",
    "                model_keywords = pd.read_csv(model_file_path, delimiter=\";\")\n",
    "                human_keywords = pd.read_csv(human_file_path)\n",
    "\n",
    "                human_set = set(preprocess_keywords(human_keywords[\"keyword\"].dropna()))\n",
    "                for column in model_keywords:\n",
    "                    model_set = set(\n",
    "                        preprocess_keywords(model_keywords[column].dropna())\n",
    "                    )\n",
    "                    tp = len(model_set & human_set)\n",
    "                    fp = len(model_set - human_set)\n",
    "                    fn = len(human_set - model_set)\n",
    "\n",
    "                    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "                    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "                    f1_score = (\n",
    "                        2 * precision * recall / (precision + recall)\n",
    "                        if precision + recall > 0\n",
    "                        else 0\n",
    "                    )\n",
    "\n",
    "                    key = (diversity, column)\n",
    "                    if key not in model_diversity_scores:\n",
    "                        model_diversity_scores[key] = []\n",
    "                    model_diversity_scores[key].append(f1_score)\n",
    "\n",
    "                count_files += 1\n",
    "\n",
    "    csv_data = []\n",
    "    for (diversity, model), scores in model_diversity_scores.items():\n",
    "        average_score = sum(scores) / len(scores)\n",
    "        csv_data.append([diversity, model, average_score])\n",
    "\n",
    "    df = pd.DataFrame(csv_data, columns=[\"Diversity Number\", \"Model name\", \"F1 score\"])\n",
    "\n",
    "    output_dir = os.path.join(output_base_dir, \"f1_keybert_scores_per_diversity\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_name = model_base_dir.split(\"/\")[-2] + \".csv\"\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    return df, count_files\n",
    "\n",
    "\n",
    "configurations = [\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for model_dir, human_dir, output_base_dir in configurations:\n",
    "    scores_df, file_count = calculate_f1_scores(model_dir, human_dir, output_base_dir)\n",
    "    print(\n",
    "        f\"Processed {file_count} files for {human_dir}. Results saved to {output_base_dir} as file {model_dir.split('/')[-1]}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc384f10-abc1-43c6-85b5-5c29b9dbf201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M1. Results saved to analytical_data/F1-score-KeyBERT/M1 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords_lemma/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n",
      "Processed 1980 files for filol_scores/keywords/philologist_M2. Results saved to analytical_data/F1-score-KeyBERT/M2 as file ngram_1_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_keywords(keywords):\n",
    "    \"\"\"Preprocess a series of keywords: lowercasing and stripping spaces.\"\"\"\n",
    "    return keywords.str.lower().str.strip()\n",
    "\n",
    "\n",
    "def calculate_f1_scores(model_base_dir, human_base_dir, output_base_dir):\n",
    "    model_scores = {}\n",
    "    count_files = 0\n",
    "    model_diversity_scores = {}\n",
    "    model_averages = {}\n",
    "    diversity_averages = {}\n",
    "\n",
    "    for diversity in range(11):\n",
    "        model_dir = os.path.join(model_base_dir, f\"diversity_{diversity}\")\n",
    "        files = [f for f in os.listdir(model_dir) if f.endswith(\".csv\")]\n",
    "        diversity_scores = []\n",
    "\n",
    "        for file_name in files:\n",
    "            model_file_path = os.path.join(model_dir, file_name)\n",
    "            human_file_path = os.path.join(human_base_dir, file_name)\n",
    "\n",
    "            if os.path.exists(human_file_path):\n",
    "                model_keywords = pd.read_csv(model_file_path, delimiter=\";\")\n",
    "                human_keywords = pd.read_csv(human_file_path)\n",
    "\n",
    "                human_set = set(preprocess_keywords(human_keywords[\"keyword\"].dropna()))\n",
    "                for column in model_keywords:\n",
    "                    model_set = set(\n",
    "                        preprocess_keywords(model_keywords[column].dropna())\n",
    "                    )\n",
    "                    tp = len(model_set & human_set)\n",
    "                    fp = len(model_set - human_set)\n",
    "                    fn = len(human_set - model_set)\n",
    "\n",
    "                    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "                    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "                    f1_score = (\n",
    "                        2 * precision * recall / (precision + recall)\n",
    "                        if precision + recall > 0\n",
    "                        else 0\n",
    "                    )\n",
    "\n",
    "                    key = (diversity, column)\n",
    "                    if key not in model_diversity_scores:\n",
    "                        model_diversity_scores[key] = []\n",
    "                    model_diversity_scores[key].append(f1_score)\n",
    "                    diversity_scores.append(f1_score)\n",
    "\n",
    "                    model_averages.setdefault(column, []).append(f1_score)\n",
    "\n",
    "                count_files += 1\n",
    "\n",
    "        if diversity_scores:\n",
    "            diversity_averages[diversity] = sum(diversity_scores) / len(\n",
    "                diversity_scores\n",
    "            )\n",
    "\n",
    "    csv_data_model = []\n",
    "    csv_data_diversity = []\n",
    "    for (diversity, model), scores in model_diversity_scores.items():\n",
    "        average_score = sum(scores) / len(scores) * 100\n",
    "        csv_data_model.append([model, average_score])\n",
    "        csv_data_diversity.append([diversity, average_score])\n",
    "\n",
    "    df_model = pd.DataFrame(csv_data_model, columns=[\"Model name\", \"F1 score\"])\n",
    "    df_diversity = pd.DataFrame(\n",
    "        csv_data_diversity, columns=[\"Diversity Number\", \"F1 score\"]\n",
    "    )\n",
    "\n",
    "    output_dir_model = os.path.join(\n",
    "        output_base_dir, \"Average_F1_Scores_across_all_files_and_diversities\"\n",
    "    )\n",
    "    output_dir_diversity = os.path.join(\n",
    "        output_base_dir, \"Average_F1_Scores_for_each_Diversity\"\n",
    "    )\n",
    "    os.makedirs(output_dir_model, exist_ok=True)\n",
    "    os.makedirs(output_dir_diversity, exist_ok=True)\n",
    "    output_file_name_model = model_base_dir.split(\"/\")[-2] + \".csv\"\n",
    "    output_file_name_diversity = \"diversity_scores.csv\"\n",
    "    output_file_path_model = os.path.join(output_dir_model, output_file_name_model)\n",
    "    output_file_path_diversity = os.path.join(\n",
    "        output_dir_diversity, output_file_name_diversity\n",
    "    )\n",
    "    df_model.to_csv(output_file_path_model, index=False)\n",
    "    df_diversity.to_csv(output_file_path_diversity, index=False)\n",
    "\n",
    "    return df_model, df_diversity, count_files\n",
    "\n",
    "\n",
    "configurations = [\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M1\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M1\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_lemma_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords_lemma/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "    (\n",
    "        \"models/unified_data_custom/raw_text_data_LCF/ngram_1_1\",\n",
    "        \"filol_scores/keywords/philologist_M2\",\n",
    "        \"analytical_data/F1-score-KeyBERT/M2\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for model_dir, human_dir, output_base_dir in configurations:\n",
    "    df_model, df_diversity, file_count = calculate_f1_scores(\n",
    "        model_dir, human_dir, output_base_dir\n",
    "    )\n",
    "    print(\n",
    "        f\"Processed {file_count} files for {human_dir}. Results saved to {output_base_dir} as file {model_dir.split('/')[-1]}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46cb29d1-72b2-4d3f-9645-44555a5c7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'analytical_data\\f1_comparison_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# SIMPEL MATHS AND TEXTRANK F1 MEASURING\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def preprocess_keywords(keywords):\n",
    "    \"\"\"Preprocess a series of keywords: lowercasing and stripping spaces.\"\"\"\n",
    "    return keywords.apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "def calculate_f1_scores(model_dir, human_dir, keyword_column):\n",
    "    total_f1_score = 0\n",
    "    files_counted = 0\n",
    "\n",
    "    if not os.path.exists(model_dir) or not os.path.exists(human_dir):\n",
    "        print(f\"Directory not found: {model_dir} or {human_dir}\")\n",
    "        return None\n",
    "\n",
    "    files = [f for f in os.listdir(model_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "    for file_name in files:\n",
    "        model_file_path = os.path.join(model_dir, file_name)\n",
    "        human_file_path = os.path.join(human_dir, file_name)\n",
    "\n",
    "        if os.path.exists(human_file_path):\n",
    "            model_keywords = pd.read_csv(model_file_path, delimiter=\";\")\n",
    "            human_keywords = pd.read_csv(human_file_path)\n",
    "\n",
    "            if \"keyword\" in human_keywords.columns:\n",
    "                human_set = set(preprocess_keywords(human_keywords[\"keyword\"].dropna()))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if keyword_column in model_keywords.columns:\n",
    "                model_set = set(\n",
    "                    preprocess_keywords(model_keywords[keyword_column].dropna())\n",
    "                )\n",
    "                tp = len(model_set & human_set)\n",
    "                fp = len(model_set - human_set)\n",
    "                fn = len(human_set - model_set)\n",
    "\n",
    "                precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "                recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "                f1_score = (\n",
    "                    2 * precision * recall / (precision + recall)\n",
    "                    if precision + recall > 0\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                total_f1_score += f1_score\n",
    "                files_counted += 1\n",
    "\n",
    "    if files_counted > 0:\n",
    "        average_f1_score = total_f1_score / files_counted\n",
    "        return average_f1_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    methods = [\"TextRank\", \"SimpleMaths\"]\n",
    "    data_types = [\"lemmas\", \"lemmas_LCF\", \"words\", \"words_LCF\"]\n",
    "    human_groups = {\n",
    "        \"lemmas\": \"keywords_lemma\",\n",
    "        \"lemmas_LCF\": \"keywords_lemma\",\n",
    "        \"words\": \"keywords\",\n",
    "        \"words_LCF\": \"keywords\",\n",
    "    }\n",
    "    model_variants = [\"M1\", \"M2\"]\n",
    "    column_names = {\n",
    "        \"TextRank\": \"Keyword\",\n",
    "        \"SimpleMaths\": {\n",
    "            \"lemmas\": \"lemma\",\n",
    "            \"lemmas_LCF\": \"lemma\",\n",
    "            \"words\": \"word\",\n",
    "            \"words_LCF\": \"word\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    output_dir = \"analytical_data\"\n",
    "    output_file = \"f1_comparison_results.csv\"\n",
    "    full_output_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory {output_dir}\")\n",
    "\n",
    "    for method in methods:\n",
    "        for data_type in data_types:\n",
    "            for variant in model_variants:\n",
    "                human_group = human_groups[data_type]\n",
    "                model_dir = f\"scores/{method}/{data_type}\"\n",
    "                human_dir = f\"filol_scores/{human_group}/philologist_{variant}\"\n",
    "                keyword_column = (\n",
    "                    column_names[method]\n",
    "                    if method == \"TextRank\"\n",
    "                    else column_names[method][data_type]\n",
    "                )\n",
    "                score = calculate_f1_scores(model_dir, human_dir, keyword_column)\n",
    "                if score is not None:\n",
    "                    result = {\n",
    "                        \"Method\": method,\n",
    "                        \"Data_Type\": data_type,\n",
    "                        \"Human_Group\": f\"{human_group}_{variant}\",\n",
    "                        \"F1 Score\": score,\n",
    "                    }\n",
    "                    results.append(result)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(full_output_path, index=False)\n",
    "    print(f\"Results saved to '{full_output_path}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ec952-a331-49d9-b52d-40c33cbfb785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
