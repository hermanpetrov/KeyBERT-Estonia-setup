{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed250282-11d3-4c26-85b1-1e15dfb84a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_and_preprocess_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = [re.sub(r'\\W+', '', line.strip().lower()) for line in file]\n",
    "    return stopwords\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = re.split(r'[\\s,.!?;:()]+', doc)\n",
    "    return [token for token in tokens if token]\n",
    "\n",
    "def get_model_embeddings(model_name):\n",
    "    if model_name in ['google/mt5-base', 'facebook/mbart-large-50']:\n",
    "        # Keep using the slow tokenizer for these models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    return TransformerDocumentEmbeddings(model_name, tokenizer=tokenizer)\n",
    "\n",
    "def extract_keywords_for_folder(folder_path, stopwords_file, output_base_folder, ngram_range, diversity, lemma_or_word):\n",
    "    stopwords = load_and_preprocess_stopwords(stopwords_file)\n",
    "    vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=ngram_range, stop_words=stopwords, token_pattern=None)\n",
    "    diversity_label = f'diversity{int(diversity * 10)}'  # Correct diversity naming\n",
    "\n",
    "    models = [\n",
    "        ('EstBERT', 'tartuNLP/EstBERT'),\n",
    "        ('LaBSE', 'sentence-transformers/LaBSE'),\n",
    "        ('mBART', 'facebook/mbart-large-50'),\n",
    "        ('mT5', 'google/mt5-base')\n",
    "    ]\n",
    "\n",
    "    for model_name, model_path in models:\n",
    "        model = get_model_embeddings(model_path)\n",
    "        model_output_folder = os.path.join(output_base_folder, model_name, lemma_or_word, f'ngram{ngram_range[0]}')\n",
    "        os.makedirs(model_output_folder, exist_ok=True)\n",
    "        kw_model = KeyBERT(model=model)\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    doc = file.read()\n",
    "\n",
    "                keywords = kw_model.extract_keywords(doc, use_mmr=True, diversity=diversity, vectorizer=vectorizer, nr_candidates=200, top_n=200)\n",
    "                csv_output_path = os.path.join(model_output_folder, file_name.replace('.txt', '.csv'))\n",
    "\n",
    "                with open(csv_output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['word', 'score'])\n",
    "                    writer.writerows(keywords)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_folder = 'raw_text'\n",
    "    lemma_folder = 'raw_text_lemma'\n",
    "    models_output_folder = 'models'\n",
    "    ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "    diversities = [0.7]\n",
    "\n",
    "    for diversity in diversities:\n",
    "        for ngram_range in ngram_ranges:\n",
    "            output_folder = os.path.join(models_output_folder, f'diversity{int(diversity * 10)}')\n",
    "            extract_keywords_for_folder(base_folder, 'estonian-stopwords.txt', output_folder, ngram_range, diversity, 'word')\n",
    "            extract_keywords_for_folder(lemma_folder, 'estonian-stopwords-lemmas.txt', output_folder, ngram_range, diversity, 'lemma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e577ea-ce10-4e30-9539-32324e01db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_and_preprocess_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = [re.sub(r'\\W+', '', line.strip().lower()) for line in file]\n",
    "    return stopwords\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = re.split(r'[\\s,.!?;:()]+', doc)\n",
    "    return [token for token in tokens if token]\n",
    "\n",
    "def get_model_embeddings(model_name):\n",
    "    if model_name in ['google/mt5-base', 'facebook/mbart-large-50']:\n",
    "        # Keep using the slow tokenizer for these models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    return TransformerDocumentEmbeddings(model_name, tokenizer=tokenizer)\n",
    "\n",
    "def extract_keywords_for_folder(folder_path, stopwords_file, output_base_folder, ngram_range, diversity, lemma_or_word):\n",
    "    stopwords = load_and_preprocess_stopwords(stopwords_file)\n",
    "    vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=ngram_range, stop_words=stopwords, token_pattern=None)\n",
    "    diversity_label = f'diversity{int(diversity * 10)}'  # Correct diversity naming\n",
    "\n",
    "    models = [\n",
    "        ('EstBERT', 'tartuNLP/EstBERT'),\n",
    "        ('LaBSE', 'sentence-transformers/LaBSE'),\n",
    "        ('mBART', 'facebook/mbart-large-50'),\n",
    "        ('mT5', 'google/mt5-base')\n",
    "    ]\n",
    "\n",
    "    for model_name, model_path in models:\n",
    "        model = get_model_embeddings(model_path)\n",
    "        model_output_folder = os.path.join(output_base_folder, model_name, lemma_or_word, f'ngram{ngram_range[0]}')\n",
    "        os.makedirs(model_output_folder, exist_ok=True)\n",
    "        kw_model = KeyBERT(model=model)\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    doc = file.read()\n",
    "\n",
    "                keywords = kw_model.extract_keywords(doc, use_mmr=True, diversity=diversity, vectorizer=vectorizer, nr_candidates=200, top_n=200)\n",
    "                csv_output_path = os.path.join(model_output_folder, file_name.replace('.txt', '.csv'))\n",
    "\n",
    "                with open(csv_output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['word', 'score'])\n",
    "                    writer.writerows(keywords)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_folder = 'raw_text'\n",
    "    lemma_folder = 'raw_text_lemma'\n",
    "    models_output_folder = 'models'\n",
    "    ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "    diversities = [0.7]\n",
    "\n",
    "    for diversity in diversities:\n",
    "        for ngram_range in ngram_ranges:\n",
    "            output_folder = os.path.join(models_output_folder, f'diversity{int(diversity * 10)}')\n",
    "            extract_keywords_for_folder(base_folder, 'estonian-stopwords.txt', output_folder, ngram_range, diversity, 'word')\n",
    "            extract_keywords_for_folder(lemma_folder, 'estonian-stopwords-lemmas.txt', output_folder, ngram_range, diversity, 'lemma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9e1d0a-77b9-434a-af06-e55d9e1bc453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\herma\\anaconda3\\envs\\FUZE\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "30ee1398-1922-4e68-8b41-bada2c7ba8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tavalise sportlasena teisisõnu', 0.4042), ('kõlab võib-olla klišee', 0.3689), ('alariksi kutse lihtne', 0.3114), ('peaasi mõnusat adrenaliini', 0.3069), ('tallinnas jõuluturniir sporditähtedele', 0.2999), ('allar levandi kuulus', 0.2894), ('mai lõpus kanaaridele', 0.2838), ('klubi hurra liikmed', 0.28), ('täis vaata raadio', 0.2645), ('11 aastat vanem', 0.2557), ('kahes stuudios ain-alar', 0.2545), ('laagrisse lanzarote saarel', 0.2541), ('havaile nii-öelda kvalifikatsiooninumbreid', 0.2529), ('ain-alar juhanson triatleet', 0.2467), ('jaanuar kodukuud ongi', 0.246), ('ain-alar õnnestus kinni', 0.2358), ('materjalid kunagine anstrunki', 0.2345), ('sissejuhatuseks täpsustus sõbrad-tuttavad', 0.228), ('ringkonna väga-väga traagiline', 0.2232), ('rokk-kontserdid täpselt meestele', 0.2227), ('hetk laiali vastus', 0.2216), ('suusatamisest lõpetan otepää', 0.2078), ('paned keha valmis', 0.2059), ('austria prantsusmaa saksamaa', 0.196), ('saateautos satub sisend', 0.1909), ('tegija võistluspaigad vaatevinklist', 0.1892), ('populaarne palmil niukseid', 0.1766), ('üritusi emotsioonibaasi hoida', 0.1724), ('ricotellaja pärit mees', 0.172), ('profiil tuul kuumus', 0.1697), ('lohutus auhind prügikastialane', 0.1686), ('ujumine 3 8', 0.1668), ('600 onju peaasi', 0.158), ('94 tiiki võistluskaal', 0.1524), ('aastani uus-meremaal tiitlit', 0.1512), ('jonnakus eestlase suurim', 0.1409), ('marko albert olümpial', 0.1334), ('25 detsember lipsas', 0.1321), ('nõus pooldavad kaitse', 0.1295), ('vaja hermen ala', 0.1256), ('42 50 maratone', 0.1234), ('distantsi testika spetsiifikat', 0.1212), ('ristandid kahtlusi kumpsime', 0.112), ('vastupidavus ala rahvamassid', 0.1116), ('vulkaanide vrakilt tegutseb', 0.1103), ('madonna laulja madonna', 0.0848), ('modereerija nimi jimi', 0.0752), ('müüakse 2200 triaatrile', 0.0635), ('hawaii ironmani 20', 0.0631), ('mk-etapp toatlemis pikast', 0.0498)]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "with open('t896.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    doc = file.read()\n",
    "def load_and_preprocess_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        stopwords = [re.sub(r'\\W+', '', line.strip().lower()) for line in file]\n",
    "    return stopwords\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    # Tokenize on spaces and punctuation but keep hyphenated words together\n",
    "    tokens = re.split(r'[\\s,.!?;:()]+', doc)\n",
    "    return [token for token in tokens if token]\n",
    "\n",
    "regular_stopwords = load_and_preprocess_stopwords('estonian-stopwords.txt')\n",
    "vectorizerx = CountVectorizer(tokenizer=custom_tokenizer,ngram_range=(3,3), stop_words=regular_stopwords, token_pattern=None)\n",
    "\n",
    "MiniLM = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "MiniLM_model = KeyBERT(model=MiniLM)\n",
    "LaBSE = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "LaBSE_model = KeyBERT(model=LaBSE)\n",
    "e5= SentenceTransformer('intfloat/multilingual-e5-large-instruct')\n",
    "e5_model = KeyBERT(model=e5)\n",
    "\n",
    "roberta_est= TransformerDocumentEmbeddings('FacebookAI/xlm-roberta-base')\n",
    "roberta_est_model = KeyBERT(model=roberta_est)\n",
    "\n",
    "estBERT= TransformerDocumentEmbeddings('tartuNLP/EstBERT')\n",
    "estBERT_model = KeyBERT(model=estBERT)\n",
    "\n",
    "keywords = LaBSE_model.extract_keywords(doc, use_mmr=True, diversity=0.7, vectorizer=vectorizerx,nr_candidates=50, top_n=50)\n",
    "\n",
    "\n",
    "keywords = LaBSE_model.extract_keywords(doc,nr_candidates=50,keyphrase_ngram_range=(1, 1), top_n=50)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73e45528-ffbd-47cc-9239-c411d2044f5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 19.15 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     run_models(docs, models, stopwords, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, diversities, ngram_ranges)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[91], line 45\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_text\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m load_and_preprocess_stopwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestonian-stopwords.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLaBSE\u001b[39m\u001b[38;5;124m'\u001b[39m: KeyBERT(model\u001b[38;5;241m=\u001b[39m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers/LaBSE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m),\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMiniLM\u001b[39m\u001b[38;5;124m'\u001b[39m: KeyBERT(model\u001b[38;5;241m=\u001b[39mSentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/paraphrase-multilingual-mpnet-base-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me5\u001b[39m\u001b[38;5;124m'\u001b[39m: KeyBERT(model\u001b[38;5;241m=\u001b[39mSentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintfloat/multilingual-e5-large-instruct\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta_est\u001b[39m\u001b[38;5;124m'\u001b[39m: KeyBERT(model\u001b[38;5;241m=\u001b[39mTransformerDocumentEmbeddings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacebookAI/xlm-roberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestBERT\u001b[39m\u001b[38;5;124m'\u001b[39m: KeyBERT(model\u001b[38;5;241m=\u001b[39mTransformerDocumentEmbeddings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtartuNLP/EstBERT\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     50\u001b[0m }\n\u001b[0;32m     51\u001b[0m docs \u001b[38;5;241m=\u001b[39m read_documents(folder_path)\n\u001b[0;32m     52\u001b[0m diversities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FUZE\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:215\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[0;32m    212\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device_name()\n\u001b[0;32m    213\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse pytorch device_name: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(device))\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault prompt name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in the configured prompts \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdictionary with keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FUZE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FUZE\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FUZE\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 802 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FUZE\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FUZE\\lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FUZE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 19.15 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def load_and_preprocess_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        stopwords = [re.sub(r'\\W+', '', line.strip().lower()) for line in file]\n",
    "    return stopwords\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    return re.split(r'[\\s,.!?;:()]+', doc)\n",
    "\n",
    "def read_documents(folder_path):\n",
    "    docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                docs.append((filename, file.read()))\n",
    "    return docs\n",
    "\n",
    "def run_models(docs, models, stopwords, output_base, diversities, ngram_ranges):\n",
    "    for diversity in diversities:\n",
    "        for ngram_range in ngram_ranges:\n",
    "            vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=ngram_range, stop_words=stopwords, token_pattern=None)\n",
    "            for model_name, model in models.items():\n",
    "                output_dir = os.path.join(output_base, f\"diversity_{diversity}\", model_name, f\"ngram_{ngram_range[0]}\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                for filename, doc in docs:\n",
    "                    output_path = os.path.join(output_dir, f\"{filename[:-4]}.csv\")\n",
    "                    keywords = model.extract_keywords(doc, use_mmr=True, diversity=diversity, vectorizer=vectorizer, nr_candidates=50, top_n=50)\n",
    "                    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                        writer = csv.writer(csvfile, delimiter=';')\n",
    "                        writer.writerow(['keyphrase', 'score'])\n",
    "                        for keyphrase, score in keywords:\n",
    "                            writer.writerow([keyphrase, score])\n",
    "\n",
    "def main():\n",
    "    folder_path = 'raw_text'\n",
    "    stopwords = load_and_preprocess_stopwords('estonian-stopwords.txt')\n",
    "    models = {\n",
    "        'LaBSE': KeyBERT(model=SentenceTransformer('sentence-transformers/LaBSE')),\n",
    "        'MiniLM': KeyBERT(model=SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')),\n",
    "        'e5': KeyBERT(model=SentenceTransformer('intfloat/multilingual-e5-large-instruct')),\n",
    "        'roberta_est': KeyBERT(model=TransformerDocumentEmbeddings('FacebookAI/xlm-roberta-base')),\n",
    "        'estBERT': KeyBERT(model=TransformerDocumentEmbeddings('tartuNLP/EstBERT'))\n",
    "    }\n",
    "    docs = read_documents(folder_path)\n",
    "    diversities = [0, 0.3, 0.7, 1]\n",
    "    ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "    run_models(docs, models, stopwords, 'models', diversities, ngram_ranges)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afedbc3-56fc-4985-8851-ec332fa601f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
