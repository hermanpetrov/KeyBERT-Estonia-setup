{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf39da-b47b-40cc-b1e6-6f2f77352756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "# Initialize Stanza pipeline for Estonian\n",
    "nlp = stanza.Pipeline(lang=\"et\")\n",
    "\n",
    "input_directory = \"raw_text/\"\n",
    "output_directory = \"raw_text_lemma\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Function to handle None values safely and remove specific characters\n",
    "def clean_text(value):\n",
    "    value = value if value is not None else \"\"\n",
    "    return re.sub(r\"[_=+]\", \"\", value)  # Remove unwanted characters\n",
    "\n",
    "# Process each file in the input directory\n",
    "for entry in os.scandir(input_directory):\n",
    "    if entry.is_file() and entry.name.endswith(\".txt\"):\n",
    "        print(\"Processing file:\", entry.name)\n",
    "        # Define the output file path\n",
    "        output_file_path = os.path.join(output_directory, entry.name)\n",
    "        with open(entry.path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "            text = input_file.read()\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "            doc = nlp(text)\n",
    "            lemmatized_text = []\n",
    "            # Extract and clean lemmas for each word in each sentence\n",
    "            for sent in doc.sentences:\n",
    "                for word in sent.words:\n",
    "                    cleaned_lemma = clean_text(word.lemma)\n",
    "                    lemmatized_text.append(cleaned_lemma)\n",
    "            # Join all cleaned lemmas with a space and write to the output file\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\" \".join(lemmatized_text) + \"\\n\")\n",
    "\n",
    "print(\"Lemmatization complete. Files written to\", output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bf58683-29d2-4a1a-af51-d44c0fd83546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing est-roberta at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing est-roberta at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing est-roberta at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing est-roberta at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing est-roberta at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing est-roberta at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=True\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (1, 1) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 0 with nr_candidates=200 and top_n=200 and lowercase=False\n",
      "Finished processing e5 at ngram range (2, 2) and diversity 1 with nr_candidates=200 and top_n=200 and lowercase=False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from torch import cuda\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def load_and_preprocess_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        stopwords = [re.sub(r'\\W+', '', line.strip().lower()) for line in file]\n",
    "    return stopwords\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    return re.split(r'[\\s,.!?;:()]+', doc)\n",
    "\n",
    "def read_documents(folder_path):\n",
    "    docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                docs.append((filename, file.read()))\n",
    "    return docs\n",
    "\n",
    "def load_model(model_info):\n",
    "    model_type, model_path = model_info\n",
    "    if model_type == 'sentence_transformer':\n",
    "        model = SentenceTransformer(model_path)\n",
    "    elif model_type == 'flair_transformer':\n",
    "        model = TransformerDocumentEmbeddings(model_path)\n",
    "    return KeyBERT(model=model)\n",
    "\n",
    "def run_models(docs, model, model_name, output_base, ngram_ranges, diversities, lowercase):\n",
    "    stopwords = load_and_preprocess_stopwords('estonian-stopwords.txt')\n",
    "    for ngram_range in ngram_ranges:\n",
    "        vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=ngram_range, stop_words=stopwords, token_pattern=None, lowercase=lowercase)\n",
    "        for diversity in diversities:\n",
    "            output_dir_path = os.path.join(output_base, f\"{model_name}\", f\"ngram_{ngram_range[0]}_{ngram_range[1]}\", f\"diversity_{int(diversity*10)}\")\n",
    "            os.makedirs(output_dir_path, exist_ok=True)\n",
    "            for filename, doc in docs:\n",
    "                output_path = os.path.join(output_dir_path, f\"{filename[:-4]}.csv\")\n",
    "                keywords = model.extract_keywords(doc, use_mmr=True, diversity=diversity, vectorizer=vectorizer, nr_candidates=200, top_n=200)\n",
    "                with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile, delimiter=';')\n",
    "                    writer.writerow(['keyphrase', 'score'])\n",
    "                    for keyphrase, score in keywords:\n",
    "                        writer.writerow([keyphrase, score])\n",
    "            print(f\"Finished processing {model_name} at ngram range {ngram_range} and diversity {diversity} with nr_candidates=200 and top_n=200 and lowercase={lowercase}\")\n",
    "    del model  # Free up memory\n",
    "    if cuda.is_available():\n",
    "        cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    base_folders = {\n",
    "        'raw_text': 'models/raw_text_data',\n",
    "        'raw_text_lemma': 'models/raw_text_lemma_data',\n",
    "    }\n",
    "    lcf_folders = {\n",
    "        'raw_text': 'models/raw_text_data_LCF',\n",
    "        'raw_text_lemma': 'models/raw_text_lemma_data_LCF'\n",
    "    }\n",
    "    models_info = {\n",
    "        'LaBSE': ('sentence_transformer', 'sentence-transformers/LaBSE'),\n",
    "        'multi_e5': ('sentence_transformer', 'intfloat/multilingual-e5-large-instruct'),\n",
    "        'MiniLM_multi': ('sentence_transformer', 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'),\n",
    "        'MiniLM-L12_multi': ('sentence_transformer', 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'),\n",
    "        'distilbertMulti': ('flair_transformer', 'distilbert/distilbert-base-multilingual-cased'),\n",
    "        'bertMulti': ('flair_transformer', 'google-bert/bert-base-multilingual-cased'),\n",
    "        'xlm-roberta': ('flair_transformer', 'FacebookAI/xlm-roberta-base'),\n",
    "        'EstBERT': ('flair_transformer', 'tartuNLP/EstBERT'),\n",
    "        'est-roberta': ('flair_transformer', 'EMBEDDIA/est-roberta')\n",
    "    }\n",
    "    ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "    diversities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "    for folder_key in base_folders:\n",
    "        folder_path = 'raw_text' if 'lemma' not in folder_key else 'raw_text_lemma'\n",
    "        docs = read_documents(folder_path)\n",
    "        for model_name, model_info in models_info.items():\n",
    "            model = load_model(model_info)\n",
    "            # Process normally\n",
    "            run_models(docs, model, model_name, base_folders[folder_key], ngram_ranges, diversities, lowercase=True)\n",
    "            # Process with lowercase=False\n",
    "            run_models(docs, model, model_name, lcf_folders[folder_key], ngram_ranges, diversities, lowercase=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883c48f3-90b1-4e56-9b7f-d101e967cf49",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'modelsx/raw_text_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m             run_models(docs, model, model_name, lcf_folders[folder_key], ngram_ranges, diversities, lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 76\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder_key \u001b[38;5;129;01min\u001b[39;00m base_folders:\n\u001b[0;32m     75\u001b[0m     folder_path \u001b[38;5;241m=\u001b[39m base_folders[folder_key]\n\u001b[1;32m---> 76\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[43mread_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_info:\n\u001b[0;32m     78\u001b[0m         model \u001b[38;5;241m=\u001b[39m load_model(model_name)\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mread_documents\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_documents\u001b[39m(folder_path):\n\u001b[0;32m     20\u001b[0m     docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     23\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'modelsx/raw_text_data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from transformers import pipeline\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def load_and_preprocess_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        stopwords = [re.sub(r'\\W+', '', line.strip().lower()) for line in file]\n",
    "    return stopwords\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    return re.split(r'[\\s,.!?;:()]+', doc)\n",
    "\n",
    "def read_documents(folder_path):\n",
    "    docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                docs.append((filename, file.read()))\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_info):\n",
    "    device = 0 if torch.cuda.is_available() else -1  # GPU device index (usually 0), -1 for CPU\n",
    "    model_name = model_info\n",
    "    hf_model = pipeline(\"feature-extraction\", model=model_name, device=device)\n",
    "    return KeyBERT(model=hf_model)\n",
    "\n",
    "\n",
    "def run_models(docs, model, model_name, output_base, ngram_ranges, diversities, lowercase):\n",
    "    stopwords = load_and_preprocess_stopwords('estonian-stopwords.txt')\n",
    "    for ngram_range in ngram_ranges:\n",
    "        vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=ngram_range, stop_words=stopwords, token_pattern=None, lowercase=lowercase)\n",
    "        for diversity in diversities:\n",
    "            output_dir_path = os.path.join(output_base, f\"{model_name}\", f\"ngram_{ngram_range[0]}_{ngram_range[1]}\", f\"diversity_{int(diversity*10)}\")\n",
    "            os.makedirs(output_dir_path, exist_ok=True)\n",
    "            for filename, doc in docs:\n",
    "                output_path = os.path.join(output_dir_path, f\"{filename[:-4]}.csv\")\n",
    "                keywords = model.extract_keywords(doc, use_mmr=True, diversity=diversity, vectorizer=vectorizer, nr_candidates=200, top_n=200)\n",
    "                with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile, delimiter=';')\n",
    "                    writer.writerow(['keyphrase', 'score'])\n",
    "                    for keyphrase, score in keywords:\n",
    "                        writer.writerow([keyphrase, score])\n",
    "            print(f\"Finished processing {model_name} at ngram range {ngram_range} and diversity {diversity} with nr_candidates=200 and top_n=200 and lowercase={lowercase}\")\n",
    "    del model  # Free up memory\n",
    "    if cuda.is_available():\n",
    "        cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    base_folders = {\n",
    "        'raw_text': 'models/raw_text_data',\n",
    "        'raw_text_lemma': 'models/raw_text_lemma_data',\n",
    "    }\n",
    "    lcf_folders = {\n",
    "        'raw_text': 'models/raw_text_data_LCF',\n",
    "        'raw_text_lemma': 'models/raw_text_lemma_data_LCF'\n",
    "    }\n",
    "    models_info = [\n",
    "        'distilbert-base-cased',\n",
    "        'bert-base-multilingual-cased',\n",
    "        'xlm-roberta-base',\n",
    "        'sentence-transformers/LaBSE'\n",
    "    ]\n",
    "    ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "    diversities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "    for folder_key in base_folders:\n",
    "        folder_path = base_folders[folder_key]\n",
    "        docs = read_documents(folder_path)\n",
    "        for model_name in models_info:\n",
    "            model = load_model(model_name)\n",
    "            # Process normally\n",
    "            run_models(docs, model, model_name, folder_path, ngram_ranges, diversities, lowercase=True)\n",
    "            # Process with lowercase=False\n",
    "            run_models(docs, model, model_name, lcf_folders[folder_key], ngram_ranges, diversities, lowercase=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a6756-48e3-4541-8f17-d43c210915ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
