{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2cd4097-00ee-421b-b5c9-7f54a4a4ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Normalized Scores by Model: {'EstBERT': 0.49772727272727274, 'est-roberta': 0.3888888888888889, 'LaBSE': 0.39562289562289554, 'bertMulti': 0.33670033670033667, 'distilbertMulti': 0.5161616161616163, 'MiniLM_multi': 0.3061447811447812, 'MiniLM-L12_multi': 0.3971380471380472, 'multi_e5': 0.48265993265993257, 'xml_roberta': 0.39789562289562286}\n",
      "Best Model: distilbertMulti\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_process_data(filenames):\n",
    "    # Combine all CSV files into a single DataFrame\n",
    "    data_frames = [pd.read_csv(filename) for filename in filenames]\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    \n",
    "    # Calculate total possible words in each row\n",
    "    combined_df['Total_Words'] = combined_df['words'].apply(lambda x: len(x.split('; ')))\n",
    "    \n",
    "    # List of model columns (edit this list based on your actual model columns)\n",
    "    model_columns = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "    \n",
    "    # Normalize scores by the total possible words for each model\n",
    "    for model in model_columns:\n",
    "        combined_df[model + '_Normalized'] = combined_df[model] / combined_df['Total_Words']\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def calculate_average_scores(df, model_columns):\n",
    "    # Calculate average normalized score for each model\n",
    "    average_scores = {model: df[model + '_Normalized'].mean() for model in model_columns}\n",
    "    return average_scores\n",
    "\n",
    "def find_best_model(average_scores):\n",
    "    # Find the model with the highest average score\n",
    "    best_model = max(average_scores, key=average_scores.get)\n",
    "    return best_model\n",
    "\n",
    "# List your CSV filenames here\n",
    "filenames = ['t887.csv', 't896.csv', 't903.csv']  # Update this list with your file paths\n",
    "\n",
    "# Processing data\n",
    "combined_df = load_and_process_data(filenames)\n",
    "\n",
    "# List of models (update this if it changes)\n",
    "model_columns = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "\n",
    "# Calculating scores\n",
    "average_scores = calculate_average_scores(combined_df, model_columns)\n",
    "best_model = find_best_model(average_scores)\n",
    "\n",
    "# Output results\n",
    "print(\"Average Normalized Scores by Model:\", average_scores)\n",
    "print(\"Best Model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33410c5-4545-4625-be6c-e4e1d5a2483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files processed: 5940\n",
      "Files not processed across all folders:\n",
      "t100098.csv\n",
      "t100101.csv\n",
      "t100218.csv\n",
      "t100506.csv\n",
      "t100648.csv\n",
      "t101044.csv\n",
      "t101345.csv\n",
      "t103140.csv\n",
      "t1064.csv\n",
      "t1189.csv\n",
      "t120660.csv\n",
      "t12088.csv\n",
      "t1423.csv\n",
      "t1505.csv\n",
      "t1637.csv\n",
      "t177159.csv\n",
      "t179339.csv\n",
      "t179429.csv\n",
      "t179494.csv\n",
      "t179497.csv\n",
      "t100098.csv\n",
      "t100101.csv\n",
      "t100218.csv\n",
      "t100506.csv\n",
      "t100648.csv\n",
      "t101044.csv\n",
      "t101345.csv\n",
      "t103140.csv\n",
      "t1064.csv\n",
      "t1189.csv\n",
      "t120660.csv\n",
      "t12088.csv\n",
      "t1423.csv\n",
      "t1505.csv\n",
      "t1637.csv\n",
      "t177159.csv\n",
      "t179339.csv\n",
      "t179429.csv\n",
      "t179494.csv\n",
      "t179497.csv\n",
      "t100098.csv\n",
      "t100101.csv\n",
      "t100218.csv\n",
      "t100506.csv\n",
      "t100648.csv\n",
      "t101044.csv\n",
      "t101345.csv\n",
      "t103140.csv\n",
      "t1064.csv\n",
      "t1189.csv\n",
      "t120660.csv\n",
      "t12088.csv\n",
      "t1423.csv\n",
      "t1505.csv\n",
      "t1637.csv\n",
      "t177159.csv\n",
      "t179339.csv\n",
      "t179429.csv\n",
      "t179494.csv\n",
      "t179497.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base and output directories\n",
    "base_path = \"./models/unified_data/raw_text_data\"\n",
    "philologist_path = \"./filol_scores/keywords/philologist_M1\"\n",
    "output_dir = \"./models/diversity_accuracy/raw_text_data\"\n",
    "\n",
    "# Ngram directories to process\n",
    "ngram_dirs = [\"ngram_1_1\", \"ngram_2_2\", \"ngram_3_3\"]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize counters and lists for tracking\n",
    "processed_files_count = 0\n",
    "unprocessed_files = []\n",
    "\n",
    "# Function to process each directory\n",
    "def process_ngram_directory(ngram_dir):\n",
    "    global processed_files_count, unprocessed_files\n",
    "    diversity_paths = [f\"{base_path}/{ngram_dir}/diversity_{i}\" for i in range(11)]\n",
    "    \n",
    "    for philologist_file in os.listdir(philologist_path):\n",
    "        if philologist_file.endswith(\".csv\"):\n",
    "            philologist_file_path = os.path.join(philologist_path, philologist_file)\n",
    "            df_philologist = pd.read_csv(philologist_file_path, delimiter=';')\n",
    "            philologist_keywords = df_philologist['keyword'].dropna().str.lower().unique()\n",
    "            expected_amount = len(philologist_keywords)  # Total number of unique expected keywords\n",
    "            model_names = [\"EstBERT\", \"est-roberta\", \"LaBSE\", \"bertMulti\", \"distilbertMulti\", \"MiniLM_multi\", \"MiniLM-L12_multi\", \"multi_e5\", \"xml_roberta\"]\n",
    "            results = {model: [0]*11 for model in model_names}\n",
    "            found_words = [{model: set() for model in model_names} for _ in range(11)]  # List of dicts to store found words uniquely per model and diversity\n",
    "\n",
    "            # Check if any matching files exist in the diversity folder\n",
    "            matching_files_exist = False\n",
    "            for idx, path in enumerate(diversity_paths):\n",
    "                diversity_file_path = os.path.join(path, philologist_file)\n",
    "                if os.path.exists(diversity_file_path):\n",
    "                    matching_files_exist = True\n",
    "                    break\n",
    "            \n",
    "            if matching_files_exist:\n",
    "                for idx, path in enumerate(diversity_paths):\n",
    "                    diversity_file_path = os.path.join(path, philologist_file)\n",
    "                    if os.path.exists(diversity_file_path):\n",
    "                        df_diversity = pd.read_csv(diversity_file_path, delimiter=';', names=model_names)\n",
    "                        for model in model_names:\n",
    "                            entries = df_diversity[model].dropna().str.lower().str.split()\n",
    "                            model_specific_words = set()\n",
    "                            for entry in entries:\n",
    "                                matched_words = set(entry) & set(philologist_keywords)\n",
    "                                model_specific_words.update(matched_words)\n",
    "                            found_words[idx][model] = model_specific_words  # Update with unique words for this model\n",
    "                            results[model][idx] = len(model_specific_words)  # Count unique words\n",
    "                        processed_files_count += 1\n",
    "\n",
    "                # Prepare output DataFrame\n",
    "                output_df = pd.DataFrame(results, index=[f\"diversity_{i}\" for i in range(11)])\n",
    "                output_df.reset_index(inplace=True)\n",
    "                output_df.rename(columns={'index': 'diversity'}, inplace=True)\n",
    "                output_df['expected_amount'] = expected_amount  # Add expected_amount as a new column\n",
    "                output_df['words'] = ['; '.join(set().union(*(d.values()))) for d in found_words]  # Combine words from all models\n",
    "\n",
    "                # Save the results\n",
    "                ngram_output_dir = os.path.join(output_dir, ngram_dir)\n",
    "                os.makedirs(ngram_output_dir, exist_ok=True)\n",
    "                output_path = os.path.join(ngram_output_dir, philologist_file)\n",
    "                output_df.to_csv(output_path, index=False)\n",
    "            else:\n",
    "                unprocessed_files.append(philologist_file)\n",
    "\n",
    "# Process each ngram directory\n",
    "for ngram_dir in ngram_dirs:\n",
    "    process_ngram_directory(ngram_dir)\n",
    "\n",
    "# Print summary of processed and unprocessed files\n",
    "print(f\"Total number of files processed: {processed_files_count}\")\n",
    "if unprocessed_files:\n",
    "    print(\"Files not processed across all folders:\")\n",
    "    for file in unprocessed_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"All files were successfully processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d4a3c87-da1b-4be0-95c5-94d4b92286d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Models per Diversity:\n",
      "diversity\n",
      "diversity_0              EstBERT\n",
      "diversity_1              EstBERT\n",
      "diversity_10            multi_e5\n",
      "diversity_2                LaBSE\n",
      "diversity_3      distilbertMulti\n",
      "diversity_4      distilbertMulti\n",
      "diversity_5      distilbertMulti\n",
      "diversity_6     MiniLM-L12_multi\n",
      "diversity_7     MiniLM-L12_multi\n",
      "diversity_8            bertMulti\n",
      "diversity_9             multi_e5\n",
      "dtype: object\n",
      "\n",
      "Best Diversities per Model:\n",
      "EstBERT             diversity_0\n",
      "est-roberta         diversity_6\n",
      "LaBSE               diversity_2\n",
      "bertMulti           diversity_8\n",
      "distilbertMulti     diversity_5\n",
      "MiniLM_multi        diversity_3\n",
      "MiniLM-L12_multi    diversity_6\n",
      "multi_e5            diversity_9\n",
      "xml_roberta         diversity_9\n",
      "dtype: object\n",
      "\n",
      "Best Overall Model and Diversity:\n",
      "('LaBSE', 'diversity_2')\n",
      "\n",
      "Diversity Rankings:\n",
      "diversity\n",
      "diversity_2     0.492593\n",
      "diversity_0     0.490741\n",
      "diversity_1     0.481481\n",
      "diversity_4     0.466667\n",
      "diversity_5     0.466667\n",
      "diversity_3     0.462037\n",
      "diversity_6     0.461111\n",
      "diversity_7     0.425926\n",
      "diversity_8     0.420370\n",
      "diversity_9     0.419444\n",
      "diversity_10    0.386111\n",
      "dtype: float64\n",
      "\n",
      "Model Rankings:\n",
      "distilbertMulti     0.531061\n",
      "MiniLM-L12_multi    0.500000\n",
      "EstBERT             0.481818\n",
      "multi_e5            0.478030\n",
      "LaBSE               0.453030\n",
      "est-roberta         0.443939\n",
      "xml_roberta         0.412121\n",
      "bertMulti           0.404545\n",
      "MiniLM_multi        0.364394\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    model_columns = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "    for column in model_columns:\n",
    "        data[column] = data[column] / data['expected_amount']\n",
    "    return data\n",
    "\n",
    "def aggregate_and_rank(files):\n",
    "    # Aggregate data from multiple files\n",
    "    aggregated_data = pd.concat([process_data(file) for file in files], ignore_index=True)\n",
    "\n",
    "    # Calculate mean scores across all entries for each model and diversity\n",
    "    mean_scores = aggregated_data.groupby('diversity')[['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']].mean()\n",
    "\n",
    "    # Best model for each diversity\n",
    "    best_models_per_diversity = mean_scores.idxmax(axis=1)\n",
    "    \n",
    "    # Best diversity for each model\n",
    "    best_diversities_per_model = mean_scores.idxmax(axis=0)\n",
    "\n",
    "    # Overall best model and diversity combination\n",
    "    best_overall_diversity = mean_scores.mean(axis=1).idxmax()\n",
    "    best_overall_model = mean_scores.loc[best_overall_diversity].idxmax()\n",
    "\n",
    "    # Ranking of diversities and models by their average scores\n",
    "    diversity_rankings = mean_scores.mean(axis=1).sort_values(ascending=False)\n",
    "    model_rankings = mean_scores.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    return {\n",
    "        'Best Models per Diversity': best_models_per_diversity,\n",
    "        'Best Diversities per Model': best_diversities_per_model,\n",
    "        'Best Overall Model and Diversity': (best_overall_model, best_overall_diversity),\n",
    "        'Diversity Rankings': diversity_rankings,\n",
    "        'Model Rankings': model_rankings\n",
    "    }\n",
    "\n",
    "# List of files to process\n",
    "files = ['t887.csv', 't903.csv']\n",
    "\n",
    "# Get results\n",
    "results = aggregate_and_rank(files)\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}:\\n{value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13378cd3-2218-47fa-8112-a60925cfaa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Models per Diversity:\n",
      "diversity\n",
      "diversity_0               LaBSE\n",
      "diversity_1               LaBSE\n",
      "diversity_10    distilbertMulti\n",
      "diversity_2               LaBSE\n",
      "diversity_3               LaBSE\n",
      "diversity_4               LaBSE\n",
      "diversity_5               LaBSE\n",
      "diversity_6     distilbertMulti\n",
      "diversity_7     distilbertMulti\n",
      "diversity_8            multi_e5\n",
      "diversity_9            multi_e5\n",
      "dtype: object\n",
      "\n",
      "Best Diversities per Model:\n",
      "EstBERT              diversity_0\n",
      "est-roberta          diversity_1\n",
      "LaBSE                diversity_0\n",
      "bertMulti            diversity_8\n",
      "distilbertMulti      diversity_7\n",
      "MiniLM_multi         diversity_4\n",
      "MiniLM-L12_multi     diversity_2\n",
      "multi_e5             diversity_9\n",
      "xml_roberta         diversity_10\n",
      "dtype: object\n",
      "\n",
      "Best Overall Model and Diversity:\n",
      "('LaBSE', 'diversity_0')\n",
      "\n",
      "Diversity Rankings:\n",
      "diversity\n",
      "diversity_0     0.248379\n",
      "diversity_2     0.248248\n",
      "diversity_1     0.247992\n",
      "diversity_3     0.243845\n",
      "diversity_4     0.241258\n",
      "diversity_8     0.240145\n",
      "diversity_6     0.239535\n",
      "diversity_7     0.237563\n",
      "diversity_5     0.237271\n",
      "diversity_9     0.236227\n",
      "diversity_10    0.234490\n",
      "dtype: float64\n",
      "\n",
      "Model Rankings:\n",
      "LaBSE               0.291990\n",
      "MiniLM-L12_multi    0.257040\n",
      "MiniLM_multi        0.255685\n",
      "distilbertMulti     0.254213\n",
      "multi_e5            0.247199\n",
      "xml_roberta         0.224457\n",
      "EstBERT             0.220578\n",
      "est-roberta         0.213125\n",
      "bertMulti           0.207946\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91d083be-cb4f-4a59-8cc1-54cc25143988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Results for ngram_1_1 ---\n",
      "\n",
      "=== Best Models per Diversity ===\n",
      "diversity\n",
      "diversity_0               LaBSE\n",
      "diversity_1               LaBSE\n",
      "diversity_10    distilbertMulti\n",
      "diversity_2               LaBSE\n",
      "diversity_3               LaBSE\n",
      "diversity_4               LaBSE\n",
      "diversity_5               LaBSE\n",
      "diversity_6     distilbertMulti\n",
      "diversity_7     distilbertMulti\n",
      "diversity_8            multi_e5\n",
      "diversity_9            multi_e5\n",
      "dtype: object\n",
      "\n",
      "=== Best Diversities per Model ===\n",
      "EstBERT              diversity_0\n",
      "est-roberta          diversity_1\n",
      "LaBSE                diversity_0\n",
      "bertMulti            diversity_8\n",
      "distilbertMulti      diversity_7\n",
      "MiniLM_multi         diversity_4\n",
      "MiniLM-L12_multi     diversity_2\n",
      "multi_e5             diversity_9\n",
      "xml_roberta         diversity_10\n",
      "dtype: object\n",
      "\n",
      "=== Best Overall Model and Diversity ===\n",
      "Best Model: LaBSE at Diversity: diversity_0\n",
      "\n",
      "=== Diversity Rankings ===\n",
      "Average normalized scores per diversity (the mean score across all models for each diversity):\n",
      "diversity\n",
      "diversity_0     0.248379\n",
      "diversity_2     0.248248\n",
      "diversity_1     0.247992\n",
      "diversity_3     0.243845\n",
      "diversity_4     0.241258\n",
      "diversity_8     0.240145\n",
      "diversity_6     0.239535\n",
      "diversity_7     0.237563\n",
      "diversity_5     0.237271\n",
      "diversity_9     0.236227\n",
      "diversity_10    0.234490\n",
      "Name: Average Normalized Score, dtype: float64\n",
      "\n",
      "=== Model Rankings ===\n",
      "Average normalized scores per model (the mean score across all diversities for each model):\n",
      "LaBSE               0.291990\n",
      "MiniLM-L12_multi    0.257040\n",
      "MiniLM_multi        0.255685\n",
      "distilbertMulti     0.254213\n",
      "multi_e5            0.247199\n",
      "xml_roberta         0.224457\n",
      "EstBERT             0.220578\n",
      "est-roberta         0.213125\n",
      "bertMulti           0.207946\n",
      "Name: Average Normalized Score, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Results for ngram_2_2 ---\n",
      "\n",
      "=== Best Models per Diversity ===\n",
      "diversity\n",
      "diversity_0      EstBERT\n",
      "diversity_1      EstBERT\n",
      "diversity_10       LaBSE\n",
      "diversity_2      EstBERT\n",
      "diversity_3      EstBERT\n",
      "diversity_4      EstBERT\n",
      "diversity_5     multi_e5\n",
      "diversity_6     multi_e5\n",
      "diversity_7     multi_e5\n",
      "diversity_8        LaBSE\n",
      "diversity_9        LaBSE\n",
      "dtype: object\n",
      "\n",
      "=== Best Diversities per Model ===\n",
      "EstBERT             diversity_3\n",
      "est-roberta         diversity_2\n",
      "LaBSE               diversity_6\n",
      "bertMulti           diversity_6\n",
      "distilbertMulti     diversity_9\n",
      "MiniLM_multi        diversity_5\n",
      "MiniLM-L12_multi    diversity_5\n",
      "multi_e5            diversity_6\n",
      "xml_roberta         diversity_9\n",
      "dtype: object\n",
      "\n",
      "=== Best Overall Model and Diversity ===\n",
      "Best Model: multi_e5 at Diversity: diversity_6\n",
      "\n",
      "=== Diversity Rankings ===\n",
      "Average normalized scores per diversity (the mean score across all models for each diversity):\n",
      "diversity\n",
      "diversity_6     0.406321\n",
      "diversity_5     0.406037\n",
      "diversity_4     0.403320\n",
      "diversity_7     0.401547\n",
      "diversity_3     0.397544\n",
      "diversity_9     0.396651\n",
      "diversity_8     0.396244\n",
      "diversity_2     0.390670\n",
      "diversity_10    0.389587\n",
      "diversity_1     0.385335\n",
      "diversity_0     0.380274\n",
      "Name: Average Normalized Score, dtype: float64\n",
      "\n",
      "=== Model Rankings ===\n",
      "Average normalized scores per model (the mean score across all diversities for each model):\n",
      "multi_e5            0.438683\n",
      "LaBSE               0.421402\n",
      "EstBERT             0.399827\n",
      "MiniLM_multi        0.398861\n",
      "bertMulti           0.391952\n",
      "MiniLM-L12_multi    0.384618\n",
      "est-roberta         0.382494\n",
      "distilbertMulti     0.374043\n",
      "xml_roberta         0.370099\n",
      "Name: Average Normalized Score, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Results for ngram_3_3 ---\n",
      "\n",
      "=== Best Models per Diversity ===\n",
      "diversity\n",
      "diversity_0      EstBERT\n",
      "diversity_1      EstBERT\n",
      "diversity_10       LaBSE\n",
      "diversity_2      EstBERT\n",
      "diversity_3      EstBERT\n",
      "diversity_4      EstBERT\n",
      "diversity_5     multi_e5\n",
      "diversity_6        LaBSE\n",
      "diversity_7        LaBSE\n",
      "diversity_8        LaBSE\n",
      "diversity_9        LaBSE\n",
      "dtype: object\n",
      "\n",
      "=== Best Diversities per Model ===\n",
      "EstBERT              diversity_3\n",
      "est-roberta          diversity_3\n",
      "LaBSE                diversity_8\n",
      "bertMulti            diversity_7\n",
      "distilbertMulti      diversity_9\n",
      "MiniLM_multi         diversity_5\n",
      "MiniLM-L12_multi     diversity_6\n",
      "multi_e5             diversity_5\n",
      "xml_roberta         diversity_10\n",
      "dtype: object\n",
      "\n",
      "=== Best Overall Model and Diversity ===\n",
      "Best Model: multi_e5 at Diversity: diversity_5\n",
      "\n",
      "=== Diversity Rankings ===\n",
      "Average normalized scores per diversity (the mean score across all models for each diversity):\n",
      "diversity\n",
      "diversity_5     0.484656\n",
      "diversity_6     0.483428\n",
      "diversity_7     0.480725\n",
      "diversity_8     0.475487\n",
      "diversity_4     0.475451\n",
      "diversity_9     0.472426\n",
      "diversity_10    0.468726\n",
      "diversity_3     0.466125\n",
      "diversity_2     0.455340\n",
      "diversity_1     0.447109\n",
      "diversity_0     0.440663\n",
      "Name: Average Normalized Score, dtype: float64\n",
      "\n",
      "=== Model Rankings ===\n",
      "Average normalized scores per model (the mean score across all diversities for each model):\n",
      "multi_e5            0.509534\n",
      "LaBSE               0.501215\n",
      "EstBERT             0.480223\n",
      "bertMulti           0.472768\n",
      "MiniLM_multi        0.465044\n",
      "est-roberta         0.462291\n",
      "MiniLM-L12_multi    0.449147\n",
      "distilbertMulti     0.446063\n",
      "xml_roberta         0.427462\n",
      "Name: Average Normalized Score, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    model_columns = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "    for column in model_columns:\n",
    "        # Normalizing the scores by the expected amount\n",
    "        data[column] = data[column] / data['expected_amount']\n",
    "    return data\n",
    "\n",
    "def aggregate_and_rank(files):\n",
    "    # Aggregate data from multiple files\n",
    "    aggregated_data = pd.concat([process_data(file) for file in files], ignore_index=True)\n",
    "\n",
    "    # Calculate mean scores across all entries for each model and diversity\n",
    "    mean_scores = aggregated_data.groupby('diversity')[model_columns].mean()\n",
    "\n",
    "    # Calculate rankings and best performances\n",
    "    best_models_per_diversity = mean_scores.idxmax(axis=1)\n",
    "    best_diversities_per_model = mean_scores.idxmax(axis=0)\n",
    "    best_overall_diversity = mean_scores.mean(axis=1).idxmax()\n",
    "    best_overall_model = mean_scores.loc[best_overall_diversity].idxmax()\n",
    "    diversity_rankings = mean_scores.mean(axis=1).sort_values(ascending=False)\n",
    "    model_rankings = mean_scores.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    return {\n",
    "        'Best Models per Diversity': best_models_per_diversity,\n",
    "        'Best Diversities per Model': best_diversities_per_model,\n",
    "        'Best Overall Model and Diversity': (best_overall_model, best_overall_diversity),\n",
    "        'Diversity Rankings': diversity_rankings,\n",
    "        'Model Rankings': model_rankings\n",
    "    }\n",
    "\n",
    "base_directory_path = 'models/diversity_accuracy/raw_text_data'\n",
    "subfolders = ['ngram_1_1', 'ngram_2_2', 'ngram_3_3']\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    print(f\"--- Results for {subfolder} ---\\n\")\n",
    "    directory_path = os.path.join(base_directory_path, subfolder)\n",
    "    files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "    results = aggregate_and_rank(files)\n",
    "\n",
    "    print(\"=== Best Models per Diversity ===\")\n",
    "    print(results['Best Models per Diversity'])\n",
    "\n",
    "    print(\"\\n=== Best Diversities per Model ===\")\n",
    "    print(results['Best Diversities per Model'])\n",
    "\n",
    "    print(\"\\n=== Best Overall Model and Diversity ===\")\n",
    "    print(f\"Best Model: {results['Best Overall Model and Diversity'][0]} at Diversity: {results['Best Overall Model and Diversity'][1]}\")\n",
    "\n",
    "    print(\"\\n=== Diversity Rankings ===\")\n",
    "    print(\"Average normalized scores per diversity (the mean score across all models for each diversity):\")\n",
    "    print(results['Diversity Rankings'].rename('Average Normalized Score'))\n",
    "\n",
    "    print(\"\\n=== Model Rankings ===\")\n",
    "    print(\"Average normalized scores per model (the mean score across all diversities for each model):\")\n",
    "    print(results['Model Rankings'].rename('Average Normalized Score'))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d45ede74-b70f-4590-b9e7-a891f58e32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Best Model per Diversity:\n",
      "diversity\n",
      "diversity_0             EstBERT\n",
      "diversity_1             EstBERT\n",
      "diversity_10    distilbertMulti\n",
      "diversity_2             EstBERT\n",
      "diversity_3             EstBERT\n",
      "diversity_4             EstBERT\n",
      "diversity_5               LaBSE\n",
      "diversity_6               LaBSE\n",
      "diversity_7               LaBSE\n",
      "diversity_8               LaBSE\n",
      "diversity_9               LaBSE\n",
      "dtype: object\n",
      "\n",
      "Statistical Numbers (Mean Scores):\n",
      "               EstBERT  est-roberta     LaBSE  bertMulti  distilbertMulti  \\\n",
      "diversity                                                                   \n",
      "diversity_0   0.422903     0.370988  0.379979   0.332841         0.303094   \n",
      "diversity_1   0.423294     0.372361  0.385398   0.333995         0.310107   \n",
      "diversity_10  0.282627     0.325807  0.402252   0.375972         0.406655   \n",
      "diversity_2   0.425702     0.373392  0.393347   0.337309         0.317320   \n",
      "diversity_3   0.423243     0.369304  0.402481   0.341431         0.326545   \n",
      "diversity_4   0.416302     0.363311  0.408960   0.343981         0.340487   \n",
      "diversity_5   0.394460     0.355397  0.415342   0.351740         0.355338   \n",
      "diversity_6   0.344731     0.346388  0.423152   0.375438         0.376730   \n",
      "diversity_7   0.315003     0.337453  0.417513   0.381144         0.393588   \n",
      "diversity_8   0.297366     0.333427  0.413804   0.382738         0.401139   \n",
      "diversity_9   0.290003     0.331173  0.411332   0.376521         0.408168   \n",
      "\n",
      "              MiniLM_multi  MiniLM-L12_multi  multi_e5  xml_roberta  \n",
      "diversity                                                            \n",
      "diversity_0       0.358961          0.350081  0.370645     0.318459  \n",
      "diversity_1       0.363092          0.354479  0.377793     0.320791  \n",
      "diversity_10      0.359133          0.350124  0.399276     0.376561  \n",
      "diversity_2       0.367629          0.358514  0.386155     0.323403  \n",
      "diversity_3       0.377101          0.362761  0.393701     0.325974  \n",
      "diversity_4       0.386591          0.371170  0.400320     0.328964  \n",
      "diversity_5       0.392724          0.379311  0.407344     0.332237  \n",
      "diversity_6       0.384772          0.375194  0.418968     0.342480  \n",
      "diversity_7       0.379131          0.369947  0.417064     0.348662  \n",
      "diversity_8       0.372530          0.365927  0.406432     0.362262  \n",
      "diversity_9       0.363498          0.362115  0.405497     0.367606  \n",
      "\n",
      "Statistical Numbers (Standard Deviations):\n",
      "               EstBERT  est-roberta     LaBSE  bertMulti  distilbertMulti  \\\n",
      "diversity                                                                   \n",
      "diversity_0   0.129551     0.128704  0.033309   0.134546         0.077460   \n",
      "diversity_1   0.134049     0.129966  0.041472   0.136581         0.079037   \n",
      "diversity_10  0.109275     0.126604  0.147954   0.115382         0.120327   \n",
      "diversity_2   0.139448     0.133635  0.055457   0.140699         0.079416   \n",
      "diversity_3   0.150156     0.134443  0.085200   0.145849         0.082613   \n",
      "diversity_4   0.154772     0.131208  0.103481   0.151985         0.087813   \n",
      "diversity_5   0.153279     0.128352  0.124490   0.155995         0.090101   \n",
      "diversity_6   0.136311     0.123981  0.138006   0.141167         0.097312   \n",
      "diversity_7   0.125543     0.122251  0.145458   0.133571         0.109120   \n",
      "diversity_8   0.120908     0.116916  0.145224   0.119041         0.118700   \n",
      "diversity_9   0.112907     0.124926  0.147639   0.118652         0.124941   \n",
      "\n",
      "              MiniLM_multi  MiniLM-L12_multi  multi_e5  xml_roberta  \n",
      "diversity                                                            \n",
      "diversity_0       0.093649          0.068964  0.127225     0.095981  \n",
      "diversity_1       0.095292          0.072429  0.135032     0.096240  \n",
      "diversity_10      0.102654          0.112752  0.113183     0.124974  \n",
      "diversity_2       0.097025          0.072720  0.140849     0.097140  \n",
      "diversity_3       0.102622          0.077885  0.148632     0.099275  \n",
      "diversity_4       0.109851          0.086501  0.157387     0.098884  \n",
      "diversity_5       0.117345          0.111499  0.157759     0.100936  \n",
      "diversity_6       0.118667          0.117768  0.145761     0.105573  \n",
      "diversity_7       0.120361          0.119991  0.134313     0.106567  \n",
      "diversity_8       0.112871          0.116707  0.119488     0.108778  \n",
      "diversity_9       0.107332          0.118416  0.113882     0.117560  \n",
      "\n",
      "Overall Best Diversity: diversity_6\n",
      "Overall Best Model at this Diversity: LaBSE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def process_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    model_columns = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "    for column in model_columns:\n",
    "        data[column] = data[column] / data['expected_amount']\n",
    "    return data\n",
    "\n",
    "def aggregate_and_rank(files):\n",
    "    # Aggregate data from multiple files\n",
    "    aggregated_data = pd.concat([process_data(file) for file in files], ignore_index=True)\n",
    "\n",
    "    # Calculate mean scores across all entries for each model and diversity\n",
    "    mean_scores = aggregated_data.groupby('diversity')[model_columns].mean()\n",
    "\n",
    "    return mean_scores\n",
    "\n",
    "# Base directory path\n",
    "base_directory_path = 'models/diversity_accuracy/raw_text_data'\n",
    "\n",
    "# Subfolder names\n",
    "subfolders = ['ngram_1_1', 'ngram_2_2', 'ngram_3_3']\n",
    "\n",
    "# Initialize a DataFrame to collect all aggregated data from each n-gram\n",
    "all_ngrams_data = pd.DataFrame()\n",
    "\n",
    "# Collect data from each subfolder\n",
    "for subfolder in subfolders:\n",
    "    directory_path = os.path.join(base_directory_path, subfolder)\n",
    "    files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "    ngram_data = aggregate_and_rank(files)\n",
    "    ngram_data['ngram'] = subfolder  # Add a column to track n-gram source\n",
    "    all_ngrams_data = pd.concat([all_ngrams_data, ngram_data], ignore_index=False)\n",
    "\n",
    "# Exclude the 'ngram' column for mean calculation\n",
    "numeric_data = all_ngrams_data.select_dtypes(include=[np.number])\n",
    "overall_mean_scores = numeric_data.groupby(numeric_data.index).mean()\n",
    "overall_std_devs = numeric_data.groupby(numeric_data.index).std()\n",
    "\n",
    "# Identify the overall best model and diversity\n",
    "best_model_per_diversity = overall_mean_scores.idxmax(axis=1)\n",
    "best_diversity = overall_mean_scores.mean(axis=1).idxmax()\n",
    "best_model = overall_mean_scores.loc[best_diversity].idxmax()\n",
    "\n",
    "# Display the results\n",
    "print(\"Overall Best Model per Diversity:\")\n",
    "print(best_model_per_diversity)\n",
    "print(\"\\nStatistical Numbers (Mean Scores):\")\n",
    "print(overall_mean_scores)\n",
    "print(\"\\nStatistical Numbers (Standard Deviations):\")\n",
    "print(overall_std_devs)\n",
    "print(\"\\nOverall Best Diversity:\", best_diversity)\n",
    "print(\"Overall Best Model at this Diversity:\", best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0653774c-e141-436e-9237-39eea6f15332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Results for ngram_1_1 ---\n",
      "\n",
      "=== Top 10 Best Model and Diversity Combinations ===\n",
      "3. Model: LaBSE at Diversity: diversity_0 with Mean Score: 0.352\n",
      "12. Model: LaBSE at Diversity: diversity_1 with Mean Score: 0.347\n",
      "30. Model: LaBSE at Diversity: diversity_2 with Mean Score: 0.340\n",
      "39. Model: LaBSE at Diversity: diversity_3 with Mean Score: 0.314\n",
      "48. Model: LaBSE at Diversity: diversity_4 with Mean Score: 0.299\n",
      "57. Model: LaBSE at Diversity: diversity_5 with Mean Score: 0.282\n",
      "98. Model: multi_e5 at Diversity: diversity_9 with Mean Score: 0.280\n",
      "34. Model: MiniLM-L12_multi at Diversity: diversity_2 with Mean Score: 0.280\n",
      "1. Model: EstBERT at Diversity: diversity_0 with Mean Score: 0.278\n",
      "43. Model: MiniLM-L12_multi at Diversity: diversity_3 with Mean Score: 0.278\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Results for ngram_2_2 ---\n",
      "\n",
      "=== Top 10 Best Model and Diversity Combinations ===\n",
      "37. Model: EstBERT at Diversity: diversity_3 with Mean Score: 0.476\n",
      "71. Model: multi_e5 at Diversity: diversity_6 with Mean Score: 0.473\n",
      "46. Model: EstBERT at Diversity: diversity_4 with Mean Score: 0.470\n",
      "28. Model: EstBERT at Diversity: diversity_2 with Mean Score: 0.470\n",
      "10. Model: EstBERT at Diversity: diversity_1 with Mean Score: 0.463\n",
      "1. Model: EstBERT at Diversity: diversity_0 with Mean Score: 0.461\n",
      "62. Model: multi_e5 at Diversity: diversity_5 with Mean Score: 0.459\n",
      "66. Model: LaBSE at Diversity: diversity_6 with Mean Score: 0.453\n",
      "80. Model: multi_e5 at Diversity: diversity_7 with Mean Score: 0.452\n",
      "53. Model: multi_e5 at Diversity: diversity_4 with Mean Score: 0.450\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Results for ngram_3_3 ---\n",
      "\n",
      "=== Top 10 Best Model and Diversity Combinations ===\n",
      "84. Model: LaBSE at Diversity: diversity_8 with Mean Score: 0.545\n",
      "75. Model: LaBSE at Diversity: diversity_7 with Mean Score: 0.545\n",
      "66. Model: LaBSE at Diversity: diversity_6 with Mean Score: 0.544\n",
      "37. Model: EstBERT at Diversity: diversity_3 with Mean Score: 0.540\n",
      "28. Model: EstBERT at Diversity: diversity_2 with Mean Score: 0.537\n",
      "46. Model: EstBERT at Diversity: diversity_4 with Mean Score: 0.537\n",
      "93. Model: LaBSE at Diversity: diversity_9 with Mean Score: 0.537\n",
      "10. Model: EstBERT at Diversity: diversity_1 with Mean Score: 0.533\n",
      "62. Model: multi_e5 at Diversity: diversity_5 with Mean Score: 0.533\n",
      "80. Model: multi_e5 at Diversity: diversity_7 with Mean Score: 0.530\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    model_columns = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "    for column in model_columns:\n",
    "        data[column] = data[column] / data['expected_amount']\n",
    "    return data\n",
    "\n",
    "def aggregate_and_rank(files):\n",
    "    # Aggregate data from multiple files\n",
    "    aggregated_data = pd.concat([process_data(file) for file in files], ignore_index=True)\n",
    "\n",
    "    # Calculate mean scores across all entries for each model and diversity\n",
    "    mean_scores = aggregated_data.groupby('diversity')[model_columns].mean()\n",
    "\n",
    "    return mean_scores\n",
    "\n",
    "def get_top_combinations(mean_scores, top_n=10):\n",
    "    # Flatten the DataFrame to have model and diversity as a MultiIndex\n",
    "    mean_scores_flat = mean_scores.stack().reset_index()\n",
    "    mean_scores_flat.columns = ['Diversity', 'Model', 'Mean Score']\n",
    "\n",
    "    # Sort the flattened DataFrame by mean scores in descending order\n",
    "    top_combinations = mean_scores_flat.sort_values(by='Mean Score', ascending=False).head(top_n)\n",
    "\n",
    "    return top_combinations\n",
    "\n",
    "base_directory_path = 'models/diversity_accuracy/raw_text_data'\n",
    "subfolders = ['ngram_1_1', 'ngram_2_2', 'ngram_3_3']\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    print(f\"--- Results for {subfolder} ---\\n\")\n",
    "    directory_path = os.path.join(base_directory_path, subfolder)\n",
    "    files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "    mean_scores = aggregate_and_rank(files)\n",
    "    \n",
    "    top_combinations = get_top_combinations(mean_scores)\n",
    "    print(\"=== Top 10 Best Model and Diversity Combinations ===\")\n",
    "    for index, row in top_combinations.iterrows():\n",
    "        print(f\"{index + 1}. Model: {row['Model']} at Diversity: {row['Diversity']} with Mean Score: {row['Mean Score']:.3f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa062e1e-2f27-4791-a44b-5505573093b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Top 10 Best Model and Diversity Combinations Across All N-grams ===\n",
      "28. Model: EstBERT at Diversity: diversity_2 with Mean Score: 0.426\n",
      "10. Model: EstBERT at Diversity: diversity_1 with Mean Score: 0.423\n",
      "37. Model: EstBERT at Diversity: diversity_3 with Mean Score: 0.423\n",
      "66. Model: LaBSE at Diversity: diversity_6 with Mean Score: 0.423\n",
      "1. Model: EstBERT at Diversity: diversity_0 with Mean Score: 0.423\n",
      "71. Model: multi_e5 at Diversity: diversity_6 with Mean Score: 0.419\n",
      "75. Model: LaBSE at Diversity: diversity_7 with Mean Score: 0.418\n",
      "80. Model: multi_e5 at Diversity: diversity_7 with Mean Score: 0.417\n",
      "46. Model: EstBERT at Diversity: diversity_4 with Mean Score: 0.416\n",
      "57. Model: LaBSE at Diversity: diversity_5 with Mean Score: 0.415\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    model_columns = ['EstBERT', 'est-roberta', 'LaBSE', 'bertMulti', 'distilbertMulti', 'MiniLM_multi', 'MiniLM-L12_multi', 'multi_e5', 'xml_roberta']\n",
    "    for column in model_columns:\n",
    "        data[column] = data[column] / data['expected_amount']\n",
    "    return data\n",
    "\n",
    "def aggregate_and_rank(files):\n",
    "    # Aggregate data from multiple files\n",
    "    aggregated_data = pd.concat([process_data(file) for file in files], ignore_index=True)\n",
    "\n",
    "    # Calculate mean scores across all entries for each model and diversity\n",
    "    mean_scores = aggregated_data.groupby('diversity')[model_columns].mean()\n",
    "\n",
    "    return mean_scores\n",
    "\n",
    "def get_top_combinations(mean_scores, top_n=10):\n",
    "    # Flatten the DataFrame to have model and diversity as a MultiIndex\n",
    "    mean_scores_flat = mean_scores.stack().reset_index()\n",
    "    mean_scores_flat.columns = ['Diversity', 'Model', 'Mean Score']\n",
    "\n",
    "    # Sort the flattened DataFrame by mean scores in descending order\n",
    "    top_combinations = mean_scores_flat.sort_values(by='Mean Score', ascending=False).head(top_n)\n",
    "\n",
    "    return top_combinations\n",
    "\n",
    "base_directory_path = 'models/diversity_accuracy/raw_text_data'\n",
    "subfolders = ['ngram_1_1', 'ngram_2_2', 'ngram_3_3']\n",
    "\n",
    "# Collect all files from all subfolders\n",
    "all_files = []\n",
    "for subfolder in subfolders:\n",
    "    directory_path = os.path.join(base_directory_path, subfolder)\n",
    "    all_files.extend([os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.csv')])\n",
    "\n",
    "# Aggregate and calculate mean scores across all n-grams\n",
    "mean_scores = aggregate_and_rank(all_files)\n",
    "\n",
    "# Get top combinations across all n-grams\n",
    "top_combinations = get_top_combinations(mean_scores)\n",
    "print(\"=== Top 10 Best Model and Diversity Combinations Across All N-grams ===\")\n",
    "for index, row in top_combinations.iterrows():\n",
    "    print(f\"{index + 1}. Model: {row['Model']} at Diversity: {row['Diversity']} with Mean Score: {row['Mean Score']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee615de-4911-486c-b891-f867e2adee7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
