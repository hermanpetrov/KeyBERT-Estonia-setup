{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a553fd-b001-4c8a-bdd1-f2567ea112e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed t152649.csv -> scores/TextRankScores\\t152649.csv\n",
      "Processed t155284.csv -> scores/TextRankScores\\t155284.csv\n",
      "Processed t157341.csv -> scores/TextRankScores\\t157341.csv\n",
      "Processed t157958.csv -> scores/TextRankScores\\t157958.csv\n",
      "Processed t158936.csv -> scores/TextRankScores\\t158936.csv\n",
      "Processed t161609.csv -> scores/TextRankScores\\t161609.csv\n",
      "Processed t162505.csv -> scores/TextRankScores\\t162505.csv\n",
      "Processed t162792.csv -> scores/TextRankScores\\t162792.csv\n",
      "Processed t164900.csv -> scores/TextRankScores\\t164900.csv\n",
      "Processed t166260.csv -> scores/TextRankScores\\t166260.csv\n",
      "Processed t887.csv -> scores/TextRankScores\\t887.csv\n",
      "Processed t896.csv -> scores/TextRankScores\\t896.csv\n",
      "Processed t903.csv -> scores/TextRankScores\\t903.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load stopwords\n",
    "with open('estonian-stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "def load_and_preprocess_data(filepath, stopwords):\n",
    "    sentences = []\n",
    "    word_details = defaultdict(dict)\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        current_sentence = []\n",
    "        for line in file:\n",
    "            parts = line.strip().split(';')\n",
    "            if parts[0] == '1' and current_sentence:  # Sentence boundary\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "            word = parts[3].lower().replace('_', '').replace('=', '').replace('+', '')\n",
    "            lemma = parts[3].replace('_', '').replace('=', '').replace('+', '')\n",
    "            upos = parts[4]\n",
    "            if upos not in ('PUNCT', 'NUM'):\n",
    "                word_key = f\"{lemma}_{parts[4]}\"\n",
    "                current_sentence.append(word_key)\n",
    "                word_details[word_key] = {\n",
    "                    'original_word': parts[2].replace('_', '').replace('=', '').replace('+', ''), \n",
    "                    'lemma': lemma, \n",
    "                    'upos': parts[4], \n",
    "                    'ner tag': parts[9]\n",
    "                }\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "    \n",
    "    # Filter out stopwords from word_details based on updated conditions\n",
    "    filtered_word_details = {}\n",
    "    for word_key, details in word_details.items():\n",
    "        word = details['original_word'].lower()\n",
    "        lemma = details['lemma']\n",
    "        # Check if both word and lemma are not in stopwords\n",
    "        if word not in stopwords or lemma.lower() not in stopwords:\n",
    "            # Post-processing for case sensitivity\n",
    "            if not any(char.isupper() for char in lemma):\n",
    "                details['original_word'] = word\n",
    "            filtered_word_details[word_key] = details\n",
    "\n",
    "    # Rebuild sentences with filtered words\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_sentence = [word_key for word_key in sentence if word_key in filtered_word_details]\n",
    "        if filtered_sentence:\n",
    "            filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    return filtered_sentences, filtered_word_details\n",
    "\n",
    "def calculate_pagerank_scores(pre_processed_dir, stopwords):\n",
    "    scores_dir = 'scores/TextRankScores'\n",
    "    os.makedirs(scores_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(pre_processed_dir):\n",
    "        if not filename.endswith('.csv'):\n",
    "            continue\n",
    "        filepath = os.path.join(pre_processed_dir, filename)\n",
    "        sentences, word_details = load_and_preprocess_data(filepath, stopwords)\n",
    "        \n",
    "        # The rest of the function remains unchanged...\n",
    "\n",
    "        \n",
    "        vocab = set([word_key for sentence in sentences for word_key in sentence])\n",
    "        vocab = {word_key: i for i, word_key in enumerate(vocab)}\n",
    "        matrix_size = len(vocab)\n",
    "        co_occurrence_matrix = np.zeros((matrix_size, matrix_size), dtype=float)\n",
    "\n",
    "        window_size = 2\n",
    "        for sentence in sentences:\n",
    "            for i, word_key in enumerate(sentence):\n",
    "                for j in range(max(i - window_size, 0), min(i + window_size + 1, len(sentence))):\n",
    "                    if i != j:\n",
    "                        co_occurrence_matrix[vocab[word_key]][vocab[sentence[j]]] += 1\n",
    "\n",
    "        graph = nx.from_numpy_array(co_occurrence_matrix)\n",
    "        scores = nx.pagerank(graph)\n",
    "\n",
    "        # Prepare the output data correctly\n",
    "        output_data = [\n",
    "            [word_details[word_key]['original_word'], word_details[word_key]['lemma'], word_details[word_key]['upos'], word_details[word_key]['ner tag'], scores[index]]\n",
    "            for word_key, index in vocab.items()\n",
    "        ]\n",
    "        \n",
    "        output_data.sort(key=lambda x: x[-1], reverse=True)  # Sort by score\n",
    "\n",
    "        # Write to CSV, changing filename extension to .csv\n",
    "        output_filename = f\"{os.path.splitext(os.path.basename(filepath))[0]}.csv\"\n",
    "        output_filepath = os.path.join(scores_dir, output_filename)\n",
    "        with open(output_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile,delimiter=';')\n",
    "            writer.writerow(['word', 'lemma', 'upos', 'ner tag', 'score'])\n",
    "            writer.writerows(output_data)\n",
    "        \n",
    "        print(f\"Processed {filename} -> {output_filepath}\")\n",
    "\n",
    "\n",
    "calculate_pagerank_scores('pre_processed_text_data', stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78132370-98a5-4d74-b4c6-6eccbfba9e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
