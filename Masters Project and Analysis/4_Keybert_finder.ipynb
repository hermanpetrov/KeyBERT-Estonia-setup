{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9edb3e-65db-492a-9822-8fc38a08ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from torch import cuda\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def load_and_preprocess_stopwords(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"UTF-8\") as file:\n",
    "        stopwords = [re.sub(r\"\\W+\", \"\", line.strip().lower()) for line in file]\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    return re.split(r\"[\\s,.!?;:()]+\", doc)\n",
    "\n",
    "\n",
    "def read_documents(folder_path):\n",
    "    docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(\n",
    "                os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                docs.append((filename, file.read()))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def load_model(model_info):\n",
    "    model_type, model_path = model_info\n",
    "    if model_type == \"sentence_transformer\":\n",
    "        model = SentenceTransformer(model_path)\n",
    "    elif model_type == \"flair_transformer\":\n",
    "        model = TransformerDocumentEmbeddings(model_path, device=\"cuda\")\n",
    "    return KeyBERT(model=model)\n",
    "\n",
    "\n",
    "def run_models(\n",
    "    docs,\n",
    "    model,\n",
    "    model_name,\n",
    "    output_base,\n",
    "    ngram_ranges,\n",
    "    diversities,\n",
    "    lowercase,\n",
    "    batch_size=5,\n",
    "):\n",
    "    stopwords = load_and_preprocess_stopwords(\"estonian-stopwords.txt\")\n",
    "    for ngram_range in ngram_ranges:\n",
    "        vectorizer = CountVectorizer(\n",
    "            tokenizer=custom_tokenizer,\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words=stopwords,\n",
    "            token_pattern=None,\n",
    "            lowercase=lowercase,\n",
    "        )\n",
    "        for diversity in diversities:\n",
    "            output_dir_path = os.path.join(\n",
    "                output_base,\n",
    "                f\"{model_name}\",\n",
    "                f\"ngram_{ngram_range[0]}_{ngram_range[1]}\",\n",
    "                f\"diversity_{int(diversity*10)}\",\n",
    "            )\n",
    "            os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "            for i in range(0, len(docs), batch_size):\n",
    "                batch_docs = docs[i : i + batch_size]\n",
    "                batch_texts = [text for _, text in batch_docs]\n",
    "                batch_filenames = [filename for filename, _ in batch_docs]\n",
    "                keywords_batch = [\n",
    "                    model.extract_keywords(\n",
    "                        doc,\n",
    "                        use_mmr=True,\n",
    "                        diversity=diversity,\n",
    "                        vectorizer=vectorizer,\n",
    "                        nr_candidates=200,\n",
    "                        top_n=200,\n",
    "                    )\n",
    "                    for doc in batch_texts\n",
    "                ]\n",
    "\n",
    "                for keywords, filename in zip(keywords_batch, batch_filenames):\n",
    "                    output_path = os.path.join(output_dir_path, f\"{filename[:-4]}.csv\")\n",
    "                    with open(\n",
    "                        output_path, \"w\", newline=\"\", encoding=\"utf-8\"\n",
    "                    ) as csvfile:\n",
    "                        writer = csv.writer(csvfile, delimiter=\";\")\n",
    "                        writer.writerow([\"keyphrase\", \"score\"])\n",
    "                        for keyphrase, score in keywords:\n",
    "                            writer.writerow([keyphrase, score])\n",
    "\n",
    "            print(\n",
    "                f\"Finished processing {model_name} at ngram range {ngram_range} and diversity {diversity} with nr_candidates=200 and top_n=200 and lowercase={lowercase}\"\n",
    "            )\n",
    "    del model\n",
    "    if cuda.is_available():\n",
    "        cuda.empty_cache()\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_folders = {\n",
    "        \"raw_text\": \"models/raw_text_data\",\n",
    "        \"raw_text_lemma\": \"models/raw_text_lemma_data\",\n",
    "    }\n",
    "    lcf_folders = {\n",
    "        \"raw_text\": \"models/raw_text_data_LCF\",\n",
    "        \"raw_text_lemma\": \"models/raw_text_lemma_data_LCF\",\n",
    "    }\n",
    "    models_info = {\n",
    "        \"LaBSE\": (\"sentence_transformer\", \"sentence-transformers/LaBSE\"),\n",
    "        \"multi_e5\": (\"sentence_transformer\", \"intfloat/multilingual-e5-large-instruct\"),\n",
    "        \"MPNet\": (\n",
    "            \"sentence_transformer\",\n",
    "            \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        ),\n",
    "        \"MiniLM-L12_multi\": (\n",
    "            \"sentence_transformer\",\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        ),\n",
    "        \"distilbertMulti\": (\n",
    "            \"flair_transformer\",\n",
    "            \"distilbert/distilbert-base-multilingual-cased\",\n",
    "        ),\n",
    "        \"bertMulti\": (\"flair_transformer\", \"google-bert/bert-base-multilingual-cased\"),\n",
    "        \"xlm-roberta\": (\"flair_transformer\", \"FacebookAI/xlm-roberta-base\"),\n",
    "        \"EstBERT\": (\"flair_transformer\", \"tartuNLP/EstBERT\"),\n",
    "        \"est-roberta\": (\"flair_transformer\", \"EMBEDDIA/est-roberta\"),\n",
    "    }\n",
    "    ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "    diversities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "    for folder_key in base_folders:\n",
    "        folder_path = \"raw_text\" if \"lemma\" not in folder_key else \"raw_text_lemma\"\n",
    "        docs = read_documents(folder_path)\n",
    "        for model_name, model_info in models_info.items():\n",
    "            model = load_model(model_info)\n",
    "            run_models(\n",
    "                docs,\n",
    "                model,\n",
    "                model_name,\n",
    "                base_folders[folder_key],\n",
    "                ngram_ranges,\n",
    "                diversities,\n",
    "                lowercase=True,\n",
    "            )\n",
    "            run_models(\n",
    "                docs,\n",
    "                model,\n",
    "                model_name,\n",
    "                lcf_folders[folder_key],\n",
    "                ngram_ranges,\n",
    "                diversities,\n",
    "                lowercase=False,\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a32098-8108-4207-af6c-a3cf6ebccf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total found files: 213840\n",
      "Total expected files: 213840\n"
     ]
    }
   ],
   "source": [
    "# RUN ONLY TO VERIFY THE FILE COUNT AFTER GETTING THE MODEL AND 4 SUBFOLDERS COMPLETED!!!! (THE EXPECTED COUNT CAN VARY FOR ME IT WAS WITH 9 MODELS) IF ANYTHING DIVIDE THE NUMBER BY 9 AND MULTIPLY BY THE MODEL AMOUNT YOU HAVE\n",
    "import os\n",
    "\n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    total_files = 0\n",
    "    for _, _, files in os.walk(directory):\n",
    "        total_files += len(files)\n",
    "    return total_files\n",
    "\n",
    "\n",
    "def verify_folder(root_dir, expected_files_per_folder, depth):\n",
    "    if depth == 0:\n",
    "        actual_files = count_files_in_directory(root_dir)\n",
    "        if actual_files != expected_files_per_folder:\n",
    "            print(\n",
    "                f\"Error: {root_dir} has {actual_files} files, expected {expected_files_per_folder}.\"\n",
    "            )\n",
    "        return actual_files\n",
    "\n",
    "    if not os.path.isdir(root_dir):\n",
    "        print(f\"Missing folder: {root_dir}\")\n",
    "        return 0\n",
    "\n",
    "    total_files = 0\n",
    "    subdirs = [\n",
    "        os.path.join(root_dir, subdir)\n",
    "        for subdir in os.listdir(root_dir)\n",
    "        if os.path.isdir(os.path.join(root_dir, subdir))\n",
    "    ]\n",
    "    if len(subdirs) == 0:\n",
    "        print(f\"Error: No subdirectories found in {root_dir}\")\n",
    "    for subdir in subdirs:\n",
    "        total_files += verify_folder(subdir, expected_files_per_folder, depth - 1)\n",
    "\n",
    "    return total_files\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_dir = \"models\"\n",
    "    depth = 4\n",
    "    expected_files_per_folder = 180\n",
    "\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Base directory {base_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    total_found_files = verify_folder(base_dir, expected_files_per_folder, depth)\n",
    "    print(f\"Total found files: {total_found_files}\")\n",
    "    expected_total_files = 4 * 9 * 3 * 11 * 180\n",
    "    print(f\"Total expected files: {expected_total_files}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
