{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad233c51-9c20-4838-89e1-dbb0eb4d6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\herma\\anaconda3\\envs\\fuze\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\herma\\anaconda3\\envs\\fuze\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae17619-4127-420a-97c6-ede2db437dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\herma\\anaconda3\\envs\\fuze\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\herma\\anaconda3\\envs\\fuze\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: Levenshtein==0.25.0 in c:\\users\\herma\\anaconda3\\envs\\fuze\\lib\\site-packages (from python-Levenshtein) (0.25.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in c:\\users\\herma\\anaconda3\\envs\\fuze\\lib\\site-packages (from Levenshtein==0.25.0->python-Levenshtein) (3.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "548145fa-576f-452c-a889-1de78f7e20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE KEYWORDS AND KEYPHRASES SEPERATIOn\n",
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "def ensure_directory_exists(path):\n",
    "    \"\"\"Ensure the specified directory exists; create it if it doesn't.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def extract_id_from_filename(filename):\n",
    "    \"\"\"Extract the ID from the filename, which is assumed to be within parentheses.\"\"\"\n",
    "    start = filename.find(\"(\")\n",
    "    end = filename.find(\")\")\n",
    "    if start != -1 and end != -1:\n",
    "        return filename[start + 1 : end]\n",
    "    return None\n",
    "\n",
    "\n",
    "def update_csv(data_frame, csv_path):\n",
    "    \"\"\"Save the DataFrame to a CSV file.\"\"\"\n",
    "    data_frame.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def generate_ngrams(words, max_size=3):\n",
    "    \"\"\"Generate n-grams up to the max_size from a list of words.\"\"\"\n",
    "    ngrams = []\n",
    "    total_words = len(words)\n",
    "    for size in range(2, max_size + 1):\n",
    "        for start in range(total_words - size + 1):\n",
    "            ngram = \" \".join(words[start : start + size])\n",
    "            ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def process_cell_content(cell_content, csv_path_keywords, csv_path_keyphrases):\n",
    "    \"\"\"Process each cell content, updating keywords and keyphrases CSV files.\"\"\"\n",
    "    keywords_data = (\n",
    "        pd.read_csv(csv_path_keywords)\n",
    "        if os.path.exists(csv_path_keywords)\n",
    "        else pd.DataFrame(columns=[\"keyword\"])\n",
    "    )\n",
    "    keyphrases_data = (\n",
    "        pd.read_csv(csv_path_keyphrases)\n",
    "        if os.path.exists(csv_path_keyphrases)\n",
    "        else pd.DataFrame(columns=[\"keyphrase\"])\n",
    "    )\n",
    "\n",
    "    words = cell_content.split()\n",
    "    all_phrases = generate_ngrams(words) if len(words) > 1 else []\n",
    "\n",
    "    if len(words) > 1:\n",
    "        full_phrase = \" \".join(words)\n",
    "        all_phrases.append(full_phrase)\n",
    "\n",
    "    for phrase in set(all_phrases):\n",
    "        if phrase not in keyphrases_data[\"keyphrase\"].values:\n",
    "            keyphrases_data = pd.concat(\n",
    "                [keyphrases_data, pd.DataFrame({\"keyphrase\": [phrase]})],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if word not in keywords_data[\"keyword\"].values:\n",
    "            keywords_data = pd.concat(\n",
    "                [keywords_data, pd.DataFrame({\"keyword\": [word]})], ignore_index=True\n",
    "            )\n",
    "\n",
    "    update_csv(keywords_data, csv_path_keywords)\n",
    "    update_csv(keyphrases_data, csv_path_keyphrases)\n",
    "\n",
    "\n",
    "def process_excel_file(file_path, source_id):\n",
    "    wb = load_workbook(filename=file_path, data_only=True)\n",
    "    sheet = wb.active\n",
    "\n",
    "    base_dir = \"filol_scores\"\n",
    "    keywords_dir = os.path.join(base_dir, \"keywords\", f\"philologist_{source_id}\")\n",
    "    keyphrases_dir = os.path.join(base_dir, \"keyphrases\", f\"philologist_{source_id}\")\n",
    "    ensure_directory_exists(keywords_dir)\n",
    "    ensure_directory_exists(keyphrases_dir)\n",
    "\n",
    "    for col in sheet.iter_cols(\n",
    "        min_row=1, max_col=sheet.max_column, min_col=1, values_only=True\n",
    "    ):\n",
    "        filename_cell = col[0]\n",
    "        if isinstance(filename_cell, str) and filename_cell.endswith(\".txt\"):\n",
    "            base_filename = filename_cell.rstrip(\".txt\")\n",
    "            csv_path_keywords = os.path.join(keywords_dir, f\"{base_filename}.csv\")\n",
    "            csv_path_keyphrases = os.path.join(keyphrases_dir, f\"{base_filename}.csv\")\n",
    "\n",
    "            for cell in col[1:]:\n",
    "                if cell is None or not isinstance(cell, str):\n",
    "                    continue\n",
    "                process_cell_content(cell, csv_path_keywords, csv_path_keyphrases)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"filol_scores/originalFiles\"\n",
    "    for file in os.listdir(base_path):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(base_path, file)\n",
    "            source_id = extract_id_from_filename(file)\n",
    "            if source_id:\n",
    "                process_excel_file(file_path, source_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afaed8a4-e10e-496b-8d94-9995eb2604a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 15:57:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 5.51MB/s]\n",
      "2024-05-04 15:57:58 INFO: Loading these models for language: et (Estonian):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | edt          |\n",
      "| lemma     | edt_nocharlm |\n",
      "============================\n",
      "\n",
      "2024-05-04 15:57:58 INFO: Using device: cuda\n",
      "2024-05-04 15:57:58 INFO: Loading: tokenize\n",
      "2024-05-04 15:57:58 INFO: Loading: lemma\n",
      "2024-05-04 15:57:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT TO CREATE LEMMATIZED VERSIONS\n",
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang=\"et\", processors=\"tokenize,lemma\")\n",
    "\n",
    "\n",
    "def ensure_directory_exists(path):\n",
    "    \"\"\"Ensure the specified directory exists; create it if it doesn't.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def extract_id_from_filename(filename):\n",
    "    \"\"\"Extract the ID from the filename, which is assumed to be within parentheses.\"\"\"\n",
    "    start = filename.find(\"(\")\n",
    "    end = filename.find(\")\")\n",
    "    if start != -1 and end != -1:\n",
    "        return filename[start + 1 : end]\n",
    "    return None\n",
    "\n",
    "\n",
    "def update_csv(data_frame, csv_path):\n",
    "    \"\"\"Save the DataFrame to a CSV file.\"\"\"\n",
    "    data_frame.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def lemmatize_content(text):\n",
    "    \"\"\"Return lemmatized versions of words and the entire text as a phrase.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def generate_ngrams(words, max_size=3):\n",
    "    \"\"\"Generate n-grams up to the max_size from a list of words.\"\"\"\n",
    "    ngrams = []\n",
    "    total_words = len(words)\n",
    "    for size in range(2, max_size + 1):\n",
    "        for start in range(total_words - size + 1):\n",
    "            ngram = \" \".join(words[start : start + size])\n",
    "            ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def process_cell_content(cell_content, csv_path_keywords, csv_path_keyphrases):\n",
    "    \"\"\"Process each cell content, updating lemmatized keywords and keyphrases CSV files.\"\"\"\n",
    "    keywords_data = (\n",
    "        pd.read_csv(csv_path_keywords)\n",
    "        if os.path.exists(csv_path_keywords)\n",
    "        else pd.DataFrame(columns=[\"keyword\"])\n",
    "    )\n",
    "    keyphrases_data = (\n",
    "        pd.read_csv(csv_path_keyphrases)\n",
    "        if os.path.exists(csv_path_keyphrases)\n",
    "        else pd.DataFrame(columns=[\"keyphrase\"])\n",
    "    )\n",
    "\n",
    "    lemmatized_words = lemmatize_content(cell_content)\n",
    "    ngrams = generate_ngrams(lemmatized_words)\n",
    "\n",
    "    if len(lemmatized_words) > 1:\n",
    "        full_phrase = \" \".join(lemmatized_words)\n",
    "        ngrams.append(full_phrase)\n",
    "\n",
    "    for ngram in set(ngrams):\n",
    "        if ngram not in keyphrases_data[\"keyphrase\"].values:\n",
    "            keyphrases_data = pd.concat(\n",
    "                [keyphrases_data, pd.DataFrame({\"keyphrase\": [ngram]})],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "    for lemma in lemmatized_words:\n",
    "        if lemma not in keywords_data[\"keyword\"].values:\n",
    "            keywords_data = pd.concat(\n",
    "                [keywords_data, pd.DataFrame({\"keyword\": [lemma]})], ignore_index=True\n",
    "            )\n",
    "\n",
    "    update_csv(keywords_data, csv_path_keywords)\n",
    "    update_csv(keyphrases_data, csv_path_keyphrases)\n",
    "\n",
    "\n",
    "def process_excel_file(file_path, source_id):\n",
    "    wb = load_workbook(filename=file_path, data_only=True)\n",
    "    sheet = wb.active\n",
    "\n",
    "    base_dir = os.path.join(\"filol_scores\")\n",
    "    keywords_dir = os.path.join(base_dir, \"keywords_lemma\", f\"philologist_{source_id}\")\n",
    "    keyphrases_dir = os.path.join(\n",
    "        base_dir, \"keyphrases_lemma\", f\"philologist_{source_id}\"\n",
    "    )\n",
    "    ensure_directory_exists(keywords_dir)\n",
    "    ensure_directory_exists(keyphrases_dir)\n",
    "\n",
    "    for col in sheet.iter_cols(\n",
    "        min_row=1, max_col=sheet.max_column, min_col=1, values_only=True\n",
    "    ):\n",
    "        filename_cell = col[0]\n",
    "        if isinstance(filename_cell, str) and filename_cell.endswith(\".txt\"):\n",
    "            base_filename = filename_cell.rstrip(\".txt\")\n",
    "            csv_path_keywords = os.path.join(keywords_dir, f\"{base_filename}.csv\")\n",
    "            csv_path_keyphrases = os.path.join(keyphrases_dir, f\"{base_filename}.csv\")\n",
    "\n",
    "            for cell in col[1:]:\n",
    "                if cell is None or not isinstance(cell, str):\n",
    "                    continue\n",
    "                process_cell_content(cell, csv_path_keywords, csv_path_keyphrases)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"filol_scores/originalFiles\"\n",
    "    for file in os.listdir(base_path):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(base_path, file)\n",
    "            source_id = extract_id_from_filename(file)\n",
    "            if source_id:\n",
    "                process_excel_file(file_path, source_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7be41b5-197a-4b9f-94c1-6eef7a36da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN _ + from lemmatization\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove specific unwanted characters from the text.\"\"\"\n",
    "    return text.replace(\"_\", \"\").replace(\"+\", \"\").replace(\"=\", \"\")\n",
    "\n",
    "\n",
    "def process_files(directory):\n",
    "    \"\"\"Process all CSV files in the specified directory to clean text fields.\"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".csv\"):\n",
    "                path = os.path.join(root, filename)\n",
    "                df = pd.read_csv(path)\n",
    "                if \"keyword\" in df.columns:\n",
    "                    df[\"keyword\"] = df[\"keyword\"].apply(clean_text)\n",
    "                if \"keyphrase\" in df.columns:\n",
    "                    df[\"keyphrase\"] = df[\"keyphrase\"].apply(clean_text)\n",
    "                df.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_directories = [\"filol_scores/keywords_lemma\", \"filol_scores/keyphrases_lemma\"]\n",
    "    for base_dir in base_directories:\n",
    "        process_files(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4d5670e-6949-407d-9466-a53f1ae18507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Count by Common Words Category:\n",
      "  1 words: 1 files, 0.56%\n",
      "  2 words: 18 files, 10.00%\n",
      "  3 words: 27 files, 15.00%\n",
      "  4 words: 26 files, 14.44%\n",
      "  5 words: 36 files, 20.00%\n",
      "  6 words: 23 files, 12.78%\n",
      "  7 words: 17 files, 9.44%\n",
      "  8 words: 7 files, 3.89%\n",
      "  9 words: 10 files, 5.56%\n",
      "  10 words: 6 files, 3.33%\n",
      "  >10 words: 8 files, 4.44%\n",
      "Philologist M1 has more words in 77 files.\n",
      "Philologist M2 has more words in 70 files.\n",
      "Both philologists have the same number of words in 33 files.\n",
      "Average number of keywords for Philologist M1: 13.24\n",
      "Average number of keywords for Philologist M2: 13.07\n",
      "Average Overlap Similarity across all files: 0.4372\n",
      "Average Jaccard Similarity across all files: 0.2732\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def read_keywords(filename):\n",
    "    with open(filename, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)\n",
    "        return {row[0].strip().lower() for row in reader if row}\n",
    "\n",
    "\n",
    "def overlap_similarity(set1, set2):\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    intersection = set1.intersection(set2)\n",
    "    smaller_set_size = min(len(set1), len(set2))\n",
    "    return len(intersection) / smaller_set_size\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 1.0\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_dir = \"filol_scores\"\n",
    "    m1_dir = os.path.join(base_dir, \"keywords/philologist_M1\")\n",
    "    m2_dir = os.path.join(base_dir, \"keywords/philologist_M2\")\n",
    "\n",
    "    similarities = {\"Overlap\": [], \"Jaccard\": []}\n",
    "    categories = {f\"{i} words\": 0 for i in range(1, 11)}\n",
    "    categories[\">10 words\"] = 0\n",
    "\n",
    "    m1_more_words = 0\n",
    "    m2_more_words = 0\n",
    "    same_word_count = 0\n",
    "\n",
    "    total_keywords_m1 = 0\n",
    "    total_keywords_m2 = 0\n",
    "\n",
    "    for filename in os.listdir(m1_dir):\n",
    "        m1_path = os.path.join(m1_dir, filename)\n",
    "        m2_path = os.path.join(m2_dir, filename)\n",
    "\n",
    "        if os.path.exists(m1_path) and os.path.exists(m2_path):\n",
    "            m1_keywords = read_keywords(m1_path)\n",
    "            m2_keywords = read_keywords(m2_path)\n",
    "            overlap_score = overlap_similarity(m1_keywords, m2_keywords)\n",
    "            jaccard_score = jaccard_similarity(m1_keywords, m2_keywords)\n",
    "            similarities[\"Overlap\"].append(overlap_score)\n",
    "            similarities[\"Jaccard\"].append(jaccard_score)\n",
    "\n",
    "            num_common = len(m1_keywords.intersection(m2_keywords))\n",
    "\n",
    "            if num_common > 10:\n",
    "                categories[\">10 words\"] += 1\n",
    "            elif num_common >= 1:\n",
    "                categories[f\"{num_common} words\"] += 1\n",
    "\n",
    "            if len(m1_keywords) > len(m2_keywords):\n",
    "                m1_more_words += 1\n",
    "            elif len(m2_keywords) > len(m1_keywords):\n",
    "                m2_more_words += 1\n",
    "            elif len(m1_keywords) == len(m2_keywords):\n",
    "                same_word_count += 1\n",
    "\n",
    "            total_keywords_m1 += len(m1_keywords)\n",
    "            total_keywords_m2 += len(m2_keywords)\n",
    "\n",
    "    total_files = 180\n",
    "    if total_files > 0:\n",
    "        print(\"File Count by Common Words Category:\")\n",
    "        for category, count in categories.items():\n",
    "            percentage = (count / total_files) * 100\n",
    "            print(f\"  {category}: {count} files, {percentage:.2f}%\")\n",
    "\n",
    "        print(f\"Philologist M1 has more words in {m1_more_words} files.\")\n",
    "        print(f\"Philologist M2 has more words in {m2_more_words} files.\")\n",
    "        print(\n",
    "            f\"Both philologists have the same number of words in {same_word_count} files.\"\n",
    "        )\n",
    "\n",
    "        avg_keywords_m1 = total_keywords_m1 / total_files\n",
    "        avg_keywords_m2 = total_keywords_m2 / total_files\n",
    "        print(f\"Average number of keywords for Philologist M1: {avg_keywords_m1:.2f}\")\n",
    "        print(f\"Average number of keywords for Philologist M2: {avg_keywords_m2:.2f}\")\n",
    "\n",
    "    if similarities[\"Overlap\"]:\n",
    "        avg_overlap = sum(similarities[\"Overlap\"]) / len(similarities[\"Overlap\"])\n",
    "        avg_jaccard = sum(similarities[\"Jaccard\"]) / len(similarities[\"Jaccard\"])\n",
    "        print(f\"Average Overlap Similarity across all files: {avg_overlap:.4f}\")\n",
    "        print(f\"Average Jaccard Similarity across all files: {avg_jaccard:.4f}\")\n",
    "    else:\n",
    "        print(\"No comparable files found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89927d4e-1b61-4b52-80fe-16912ac1d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Jaccard Similarity across all files: 0.2970\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def read_keywords(filename):\n",
    "    with open(filename, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip the header\n",
    "        return {row[0].strip().lower() for row in reader if row}\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 1.0  # If both sets are empty, define similarity as 1\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_dir = \"filol_scores\"\n",
    "    m1_dir = os.path.join(base_dir, \"keywords_lemma/philologist_M1\")\n",
    "    m2_dir = os.path.join(base_dir, \"keywords_lemma/philologist_M2\")\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for filename in os.listdir(m1_dir):\n",
    "        m1_path = os.path.join(m1_dir, filename)\n",
    "        m2_path = os.path.join(m2_dir, filename)\n",
    "\n",
    "        if os.path.exists(m1_path) and os.path.exists(m2_path):\n",
    "            m1_keywords = read_keywords(m1_path)\n",
    "            m2_keywords = read_keywords(m2_path)\n",
    "            similarity = jaccard_similarity(m1_keywords, m2_keywords)\n",
    "            similarities.append(similarity)\n",
    "            # print(f\"Jaccard Similarity for {filename}: {similarity:.4f}\")\n",
    "\n",
    "    if similarities:\n",
    "        average_similarity = sum(similarities) / len(similarities)\n",
    "        print(f\"Average Jaccard Similarity across all files: {average_similarity:.4f}\")\n",
    "    else:\n",
    "        print(\"No comparable files found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc77ffe-7926-4941-867d-6f7f51bf35bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
