{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e48f6b-bf8a-4156-bd7f-043bcf89d816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been processed and saved with summed counts for identical entries, sorted by count.\n"
     ]
    }
   ],
   "source": [
    "# CREATES LOWERCASE REFERENCE CORPUS VERSION  LOWER CASE TRUE\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "input_dir = \"SimpleMaths/corpus/reference\"\n",
    "output_dir = \"SimpleMaths/corpus/reference_LCT\"\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, \"lemmas\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"words\"), exist_ok=True)\n",
    "\n",
    "\n",
    "def process_csv(file_path, output_subdir):\n",
    "    data = pd.read_csv(os.path.join(input_dir, file_path))\n",
    "    data.iloc[:, 0] = data.iloc[:, 0].str.lower()\n",
    "    data = data.groupby(data.columns[0], as_index=False).agg({data.columns[1]: \"sum\"})\n",
    "    data = data.sort_values(by=data.columns[1], ascending=False)\n",
    "    data.to_csv(\n",
    "        os.path.join(output_dir, output_subdir, os.path.basename(file_path)),\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "\n",
    "process_csv(\"lemmas/lemmas.csv\", \"lemmas\")\n",
    "process_csv(\"words/words.csv\", \"words\")\n",
    "print(\n",
    "    \"CSV files have been processed and saved with summed counts for identical entries, sorted by count.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1250e9be-1a89-4613-bdab-57c9a874603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 19:53:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 4.58MB/s]\n",
      "2024-05-04 19:53:28 INFO: Loading these models for language: et (Estonian):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | edt          |\n",
      "| pos       | edt_nocharlm |\n",
      "| lemma     | edt_nocharlm |\n",
      "| depparse  | edt_nocharlm |\n",
      "============================\n",
      "\n",
      "2024-05-04 19:53:28 INFO: Using device: cuda\n",
      "2024-05-04 19:53:28 INFO: Loading: tokenize\n",
      "2024-05-04 19:53:28 INFO: Loading: pos\n",
      "2024-05-04 19:53:28 INFO: Loading: lemma\n",
      "2024-05-04 19:53:28 INFO: Loading: depparse\n",
      "2024-05-04 19:53:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: t10352.txt\n",
      "Processing file: t105779.txt\n",
      "Processing file: t105808.txt\n",
      "Processing file: t106205.txt\n",
      "Processing file: t106285.txt\n",
      "Processing file: t106306.txt\n",
      "Processing file: t106434.txt\n",
      "Processing file: t106764.txt\n",
      "Processing file: t10801.txt\n",
      "Processing file: t10878.txt\n",
      "Processing file: t109127.txt\n",
      "Processing file: t10948.txt\n",
      "Processing file: t110581.txt\n",
      "Processing file: t112542.txt\n",
      "Processing file: t1134.txt\n",
      "Processing file: t114676.txt\n",
      "Processing file: t115737.txt\n",
      "Processing file: t116111.txt\n",
      "Processing file: t116480.txt\n",
      "Processing file: t120240.txt\n",
      "Processing file: t121275.txt\n",
      "Processing file: t126269.txt\n",
      "Processing file: t12676.txt\n",
      "Processing file: t127878.txt\n",
      "Processing file: t128045.txt\n",
      "Processing file: t129828.txt\n",
      "Processing file: t131838.txt\n",
      "Processing file: t134839.txt\n",
      "Processing file: t135605.txt\n",
      "Processing file: t136064.txt\n",
      "Processing file: t136798.txt\n",
      "Processing file: t137720.txt\n",
      "Processing file: t138410.txt\n",
      "Processing file: t138675.txt\n",
      "Processing file: t139110.txt\n",
      "Processing file: t140638.txt\n",
      "Processing file: t14326.txt\n",
      "Processing file: t144627.txt\n",
      "Processing file: t145291.txt\n",
      "Processing file: t145304.txt\n",
      "Processing file: t145897.txt\n",
      "Processing file: t146636.txt\n",
      "Processing file: t146888.txt\n",
      "Processing file: t147619.txt\n",
      "Processing file: t14851.txt\n",
      "Processing file: t149439.txt\n",
      "Processing file: t150298.txt\n",
      "Processing file: t150517.txt\n",
      "Processing file: t150690.txt\n",
      "Processing file: t152649.txt\n",
      "Processing file: t15333.txt\n",
      "Processing file: t155284.txt\n",
      "Processing file: t157341.txt\n",
      "Processing file: t15790.txt\n",
      "Processing file: t157958.txt\n",
      "Processing file: t15824.txt\n",
      "Processing file: t158936.txt\n",
      "Processing file: t159883.txt\n",
      "Processing file: t161609.txt\n",
      "Processing file: t162180.txt\n",
      "Processing file: t162505.txt\n",
      "Processing file: t162792.txt\n",
      "Processing file: t163990.txt\n",
      "Processing file: t16448.txt\n",
      "Processing file: t164806.txt\n",
      "Processing file: t164900.txt\n",
      "Processing file: t166260.txt\n",
      "Processing file: t167448.txt\n",
      "Processing file: t167683.txt\n",
      "Processing file: t169138.txt\n",
      "Processing file: t170000.txt\n",
      "Processing file: t173278.txt\n",
      "Processing file: t173321.txt\n",
      "Processing file: t174842.txt\n",
      "Processing file: t175223.txt\n",
      "Processing file: t176309.txt\n",
      "Processing file: t176641.txt\n",
      "Processing file: t177151.txt\n",
      "Processing file: t177461.txt\n",
      "Processing file: t177941.txt\n",
      "Processing file: t178465.txt\n",
      "Processing file: t178473.txt\n",
      "Processing file: t178832.txt\n",
      "Processing file: t179464.txt\n",
      "Processing file: t18135.txt\n",
      "Processing file: t18307.txt\n",
      "Processing file: t18677.txt\n",
      "Processing file: t1952.txt\n",
      "Processing file: t20410.txt\n",
      "Processing file: t20988.txt\n",
      "Processing file: t21182.txt\n",
      "Processing file: t22465.txt\n",
      "Processing file: t22731.txt\n",
      "Processing file: t22919.txt\n",
      "Processing file: t22949.txt\n",
      "Processing file: t24614.txt\n",
      "Processing file: t26363.txt\n",
      "Processing file: t26726.txt\n",
      "Processing file: t28681.txt\n",
      "Processing file: t3007.txt\n",
      "Processing file: t30749.txt\n",
      "Processing file: t30857.txt\n",
      "Processing file: t3198.txt\n",
      "Processing file: t33123.txt\n",
      "Processing file: t33125.txt\n",
      "Processing file: t33364.txt\n",
      "Processing file: t33882.txt\n",
      "Processing file: t34181.txt\n",
      "Processing file: t34414.txt\n",
      "Processing file: t35252.txt\n",
      "Processing file: t3569.txt\n",
      "Processing file: t36565.txt\n",
      "Processing file: t38136.txt\n",
      "Processing file: t38324.txt\n",
      "Processing file: t38497.txt\n",
      "Processing file: t38770.txt\n",
      "Processing file: t39031.txt\n",
      "Processing file: t39452.txt\n",
      "Processing file: t40644.txt\n",
      "Processing file: t4073.txt\n",
      "Processing file: t4264.txt\n",
      "Processing file: t42742.txt\n",
      "Processing file: t42964.txt\n",
      "Processing file: t43448.txt\n",
      "Processing file: t45258.txt\n",
      "Processing file: t45442.txt\n",
      "Processing file: t46717.txt\n",
      "Processing file: t47984.txt\n",
      "Processing file: t48205.txt\n",
      "Processing file: t48291.txt\n",
      "Processing file: t48649.txt\n",
      "Processing file: t50591.txt\n",
      "Processing file: t5189.txt\n",
      "Processing file: t51941.txt\n",
      "Processing file: t52108.txt\n",
      "Processing file: t55241.txt\n",
      "Processing file: t55395.txt\n",
      "Processing file: t55779.txt\n",
      "Processing file: t55815.txt\n",
      "Processing file: t56373.txt\n",
      "Processing file: t58914.txt\n",
      "Processing file: t61453.txt\n",
      "Processing file: t62021.txt\n",
      "Processing file: t6264.txt\n",
      "Processing file: t66833.txt\n",
      "Processing file: t67906.txt\n",
      "Processing file: t68018.txt\n",
      "Processing file: t68062.txt\n",
      "Processing file: t68318.txt\n",
      "Processing file: t68573.txt\n",
      "Processing file: t69066.txt\n",
      "Processing file: t69382.txt\n",
      "Processing file: t71586.txt\n",
      "Processing file: t7207.txt\n",
      "Processing file: t73156.txt\n",
      "Processing file: t7370.txt\n",
      "Processing file: t75609.txt\n",
      "Processing file: t7575.txt\n",
      "Processing file: t75999.txt\n",
      "Processing file: t79065.txt\n",
      "Processing file: t7974.txt\n",
      "Processing file: t81380.txt\n",
      "Processing file: t81518.txt\n",
      "Processing file: t8330.txt\n",
      "Processing file: t84533.txt\n",
      "Processing file: t8547.txt\n",
      "Processing file: t887.txt\n",
      "Processing file: t896.txt\n",
      "Processing file: t90219.txt\n",
      "Processing file: t903.txt\n",
      "Processing file: t90710.txt\n",
      "Processing file: t9163.txt\n",
      "Processing file: t92023.txt\n",
      "Processing file: t9230.txt\n",
      "Processing file: t93033.txt\n",
      "Processing file: t94662.txt\n",
      "Processing file: t97194.txt\n",
      "Processing file: t972.txt\n",
      "Processing file: t98806.txt\n",
      "Processing file: t9917.txt\n",
      "Processing complete. Data written to the output directory.\n"
     ]
    }
   ],
   "source": [
    "# CREATES PRE_PROCESSED_TEXT_DATA_FOLDER\n",
    "import os\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang=\"et\", use_gpu=True)\n",
    "feat_labels = [\n",
    "    \"file\",\n",
    "    \"sent id\",\n",
    "    \"word id\",\n",
    "    \"word\",\n",
    "    \"lemma\",\n",
    "    \"upos\",\n",
    "    \"deprel\",\n",
    "    \"head\",\n",
    "    \"head upos\",\n",
    "    \"head id\",\n",
    "    \"ner tag\",\n",
    "    \"sent text\",\n",
    "]\n",
    "input_directory = \"raw_text/\"\n",
    "output_directory = \"pre_processed_text_data\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "\n",
    "def handle_none(value):\n",
    "    \"\"\"Return the provided value or 'None' if the value is None\"\"\"\n",
    "    return value if value is not None else \"None\"\n",
    "\n",
    "\n",
    "for entry in os.scandir(input_directory):\n",
    "    if entry.is_file() and entry.name.endswith(\".txt\"):\n",
    "        print(\"Processing file:\", entry.name)\n",
    "        output_file_path = os.path.join(output_directory, entry.name[:-4] + \".csv\")\n",
    "\n",
    "        with open(entry.path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "            text = input_file.read()\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "            doc = nlp(text)\n",
    "            sentence_id = 0\n",
    "\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as output:\n",
    "                output.write(\";\".join(feat_labels[1:-1]) + \"\\n\")\n",
    "                for sent in doc.sentences:\n",
    "                    sentence_id += 1\n",
    "                    for word in sent.words:\n",
    "                        word_data = [\n",
    "                            entry.name[:-4],\n",
    "                            str(sentence_id),\n",
    "                            str(word.id),\n",
    "                            word.text,\n",
    "                            handle_none(word.lemma),\n",
    "                            word.upos,\n",
    "                            word.deprel,\n",
    "                            sent.words[word.head - 1].text if word.head > 0 else \"root\",\n",
    "                            sent.words[word.head - 1].upos if word.head > 0 else \"_\",\n",
    "                            str(word.head),\n",
    "                            \"O\",\n",
    "                        ]\n",
    "                        output.write(\";\".join(word_data[1:]) + \"\\n\")\n",
    "\n",
    "print(\"Processing complete. Data written to the output directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b97ada-8ef6-4f71-ab79-55fcb2938f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete. All CSV files have been processed.\n"
     ]
    }
   ],
   "source": [
    "# REMOVE THE = _ + FROM THE WORDS CLEAN STRINGS\n",
    "output_directory = \"pre_processed_text_data\"\n",
    "\n",
    "\n",
    "def clean_line(line):\n",
    "\n",
    "    return re.sub(r\"[+=_]\", \"\", line)\n",
    "\n",
    "\n",
    "for entry in os.scandir(output_directory):\n",
    "    if entry.is_file() and entry.name.endswith(\".csv\"):\n",
    "\n",
    "        with open(entry.path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        cleaned_lines = [clean_line(line) for line in lines]\n",
    "\n",
    "        with open(entry.path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(cleaned_lines)\n",
    "\n",
    "print(\"Cleaning complete. All CSV files have been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b4111e-978a-44c4-a6f3-2e84c16742ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAG THE STOPWORDS\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def load_stopwords(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        return set(word.strip().lower() for word in f)\n",
    "\n",
    "\n",
    "def process_files(\n",
    "    source_directory, destination_directory, stopwords, stopwords_lemmas, column\n",
    "):\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            counts = {}\n",
    "            with open(\n",
    "                os.path.join(source_directory, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                reader = csv.DictReader(file, delimiter=\";\")\n",
    "                for row in reader:\n",
    "                    original_value = row[column]\n",
    "                    lemma = row.get(\"lemma\", \"\")\n",
    "\n",
    "                    if lemma.islower() and original_value[0].isupper():\n",
    "                        processed_value = original_value.lower()\n",
    "                    else:\n",
    "                        processed_value = original_value\n",
    "\n",
    "                    processed_key_value = processed_value.replace(\"_\", \"\").replace(\n",
    "                        \"=\", \"\"\n",
    "                    )\n",
    "\n",
    "                    key = (\n",
    "                        processed_value,\n",
    "                        row[\"upos\"],\n",
    "                        row[\"ner tag\"],\n",
    "                        processed_key_value.lower(),\n",
    "                    )\n",
    "                    counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "            with open(\n",
    "                os.path.join(destination_directory, filename),\n",
    "                \"w\",\n",
    "                encoding=\"utf-8\",\n",
    "                newline=\"\",\n",
    "            ) as outfile:\n",
    "                writer = csv.writer(outfile, delimiter=\";\", lineterminator=\"\\n\")\n",
    "                writer.writerow([column, \"upos\", \"ner tag\", \"count\", \"stopword\"])\n",
    "                for key, count in counts.items():\n",
    "\n",
    "                    is_stopword = (\n",
    "                        \"yes\"\n",
    "                        if key[3] in stopwords or key[3] in stopwords_lemmas\n",
    "                        else \"no\"\n",
    "                    )\n",
    "                    writer.writerow([key[0], key[1], key[2], count, is_stopword])\n",
    "\n",
    "\n",
    "source_directory = \"pre_processed_text_data\"\n",
    "destination_directory_words = \"SimpleMaths/corpus/focus/words\"\n",
    "destination_directory_lemmas = \"SimpleMaths/corpus/focus/lemmas\"\n",
    "\n",
    "stopwords = load_stopwords(\"estonian-stopwords.txt\")\n",
    "stopwords_lemmas = load_stopwords(\"estonian-stopwords-lemmas.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4354ae9-033e-44a0-b96a-a4ea9ee80fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference data from: SimpleMaths\\corpus\\reference\\lemmas\\lemmas.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lemmas files: 100%|███████████████████████████████████████████████████████| 180/180 [01:13<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference data from: SimpleMaths\\corpus\\reference\\words\\words.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words files: 100%|████████████████████████████████████████████████████████| 180/180 [12:00<00:00,  4.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# CREATE KEYNESSDATA FOR CALCULATION WITH SIMPLEMATHS NOT LOWERCASED\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_corpus_data(base_dir, data_type):\n",
    "    base_dir_path = Path(base_dir)\n",
    "\n",
    "    column_name = \"lemma\" if data_type == \"lemmas\" else \"word\"\n",
    "\n",
    "    reference_path = (\n",
    "        base_dir_path / \"corpus\" / \"reference\" / data_type / f\"{data_type}.csv\"\n",
    "    )\n",
    "    print(f\"Loading reference data from: {reference_path}\")\n",
    "    reference_df = pd.read_csv(reference_path, sep=\",\")\n",
    "\n",
    "    reference_df[column_name] = (\n",
    "        reference_df[column_name]\n",
    "        .str.replace(\"_\", \"\")\n",
    "        .replace(\"+\", \"\")\n",
    "        .replace(\"=\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    reference_dict = pd.Series(\n",
    "        reference_df[\"count\"].values, index=reference_df[column_name]\n",
    "    ).to_dict()\n",
    "\n",
    "    rfcTotalCount = reference_df[\"count\"].sum()\n",
    "\n",
    "    keyness_dir = base_dir_path / \"keynessData\" / data_type\n",
    "    keyness_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    focus_dir = base_dir_path / \"corpus\" / \"focus\" / data_type\n",
    "    focus_files = list(focus_dir.glob(\"*.csv\"))\n",
    "\n",
    "    for file_path in tqdm(focus_files, desc=f\"Processing {data_type} files\"):\n",
    "        focus_df = pd.read_csv(file_path, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "        focus_df[column_name] = (\n",
    "            focus_df[column_name]\n",
    "            .str.replace(\"_\", \"\")\n",
    "            .replace(\"+\", \"\")\n",
    "            .replace(\"=\", \"\", regex=True)\n",
    "        )\n",
    "\n",
    "        fcTotalCount = focus_df[\"count\"].sum()\n",
    "        focus_df[\"rfc_count\"] = (\n",
    "            focus_df[column_name].map(reference_dict).fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "        focus_df[\"fcTotalCount\"] = fcTotalCount\n",
    "        focus_df[\"rfcTotalCount\"] = rfcTotalCount\n",
    "        focus_df.rename(columns={\"count\": \"fc_count\"}, inplace=True)\n",
    "\n",
    "        output_file_path = keyness_dir / file_path.name\n",
    "        focus_df.to_csv(output_file_path, index=False, sep=\";\")\n",
    "\n",
    "\n",
    "base_dir = \"SimpleMaths\"\n",
    "process_corpus_data(base_dir, \"lemmas\")\n",
    "process_corpus_data(base_dir, \"words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8088dc90-e51a-4f57-9e5d-0ea58ef69b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference data from: SimpleMaths\\corpus\\reference_LCT\\lemmas\\lemmas.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lemmas files: 100%|███████████████████████████████████████████████████████| 180/180 [01:11<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference data from: SimpleMaths\\corpus\\reference_LCT\\words\\words.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words files: 100%|████████████████████████████████████████████████████████| 180/180 [10:33<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# CREATE KEYNESSDATA FOR CALCULATION WITH SIMPLEMATHS NOT LOWERCASED (LOWERCASES THE FOCUS CORPUS DATA AND USES LCT REFERENCE CORPUS)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_corpus_data(base_dir, data_type):\n",
    "    base_dir_path = Path(base_dir)\n",
    "\n",
    "    column_name = \"lemma\" if data_type == \"lemmas\" else \"word\"\n",
    "\n",
    "    reference_path = (\n",
    "        base_dir_path / \"corpus\" / \"reference_LCT\" / data_type / f\"{data_type}.csv\"\n",
    "    )\n",
    "    print(f\"Loading reference data from: {reference_path}\")\n",
    "    reference_df = pd.read_csv(reference_path, sep=\",\")\n",
    "\n",
    "    reference_df[column_name] = (\n",
    "        reference_df[column_name]\n",
    "        .str.replace(\"_\", \"\")\n",
    "        .replace(\"+\", \"\")\n",
    "        .replace(\"=\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    reference_dict = pd.Series(\n",
    "        reference_df[\"count\"].values, index=reference_df[column_name].str.lower()\n",
    "    ).to_dict()\n",
    "\n",
    "    rfcTotalCount = reference_df[\"count\"].sum()\n",
    "\n",
    "    keyness_dir = base_dir_path / \"keynessData_LCT\" / data_type\n",
    "    keyness_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    focus_dir = base_dir_path / \"corpus\" / \"focus\" / data_type\n",
    "    focus_files = list(focus_dir.glob(\"*.csv\"))\n",
    "\n",
    "    for file_path in tqdm(focus_files, desc=f\"Processing {data_type} files\"):\n",
    "        focus_df = pd.read_csv(file_path, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "        focus_df[column_name] = (\n",
    "            focus_df[column_name]\n",
    "            .str.lower()\n",
    "            .str.replace(\"_\", \"\")\n",
    "            .replace(\"+\", \"\")\n",
    "            .replace(\"=\", \"\", regex=True)\n",
    "        )\n",
    "\n",
    "        fcTotalCount = focus_df[\"count\"].sum()\n",
    "        focus_df[\"rfc_count\"] = (\n",
    "            focus_df[column_name].map(reference_dict).fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "        focus_df[\"fcTotalCount\"] = fcTotalCount\n",
    "        focus_df[\"rfcTotalCount\"] = rfcTotalCount\n",
    "        focus_df.rename(columns={\"count\": \"fc_count\"}, inplace=True)\n",
    "\n",
    "        output_file_path = keyness_dir / file_path.name\n",
    "        focus_df.to_csv(output_file_path, index=False, sep=\";\")\n",
    "\n",
    "\n",
    "base_dir = \"SimpleMaths\"\n",
    "process_corpus_data(base_dir, \"lemmas\")\n",
    "process_corpus_data(base_dir, \"words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8714e786-2156-4486-8ef7-666d0694073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been updated with simpleMathsScore.\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE SIMPLEMATHS VALUE TO THE CSV IN KEYNESSDATA\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_simpleMathsScore(df):\n",
    "\n",
    "    df[\"rfc_count\"] = df[\"rfc_count\"].replace(0, 1)\n",
    "\n",
    "    df[\"fc_per_million_hits\"] = (df[\"fc_count\"] * 1000000) / df[\"fcTotalCount\"]\n",
    "    df[\"rfc_per_million_hits\"] = (df[\"rfc_count\"] * 1000000) / df[\"rfcTotalCount\"]\n",
    "\n",
    "    df[\"simpleMathsScore\"] = (df[\"fc_per_million_hits\"] + 1) / (\n",
    "        df[\"rfc_per_million_hits\"] + 1\n",
    "    )\n",
    "\n",
    "    df.drop(columns=[\"fc_per_million_hits\", \"rfc_per_million_hits\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "lemmas_dir = \"SimpleMaths/keynessData/lemmas\"  ### ADJUST HERE FOR LCT OR REGULAR\n",
    "words_dir = \"SimpleMaths/keynessData/words\"\n",
    "\n",
    "# Process all CSV files in the lemmas directory\n",
    "for filename in os.listdir(lemmas_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(lemmas_dir, filename)\n",
    "        df = pd.read_csv(file_path, delimiter=\";\")\n",
    "        updated_df = calculate_simpleMathsScore(df)\n",
    "        updated_df.to_csv(file_path, index=False, sep=\";\")\n",
    "\n",
    "for filename in os.listdir(words_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(words_dir, filename)\n",
    "        df = pd.read_csv(file_path, delimiter=\";\")\n",
    "        updated_df = calculate_simpleMathsScore(df)\n",
    "        updated_df.to_csv(file_path, index=False, sep=\";\")\n",
    "\n",
    "print(\"All files have been updated with simpleMathsScore.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d888ac52-6a77-455c-a0a3-257b85f37402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been updated with simpleMathsScore.\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE SIMPLEMATSH FOR LOWER CASE TRUE DATA\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_simpleMathsScore(df):\n",
    "\n",
    "    df[\"rfc_count\"] = df[\"rfc_count\"].replace(0, 1)\n",
    "\n",
    "    df[\"fc_per_million_hits\"] = (df[\"fc_count\"] * 1000000) / df[\"fcTotalCount\"]\n",
    "    df[\"rfc_per_million_hits\"] = (df[\"rfc_count\"] * 1000000) / df[\"rfcTotalCount\"]\n",
    "\n",
    "    df[\"simpleMathsScore\"] = (df[\"fc_per_million_hits\"] + 1) / (\n",
    "        df[\"rfc_per_million_hits\"] + 1\n",
    "    )\n",
    "\n",
    "    df.drop(columns=[\"fc_per_million_hits\", \"rfc_per_million_hits\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "lemmas_dir = \"SimpleMaths/keynessData_LCT/lemmas\"  ### ADJUST HERE FOR LCT OR REGULAR\n",
    "words_dir = \"SimpleMaths/keynessData_LCT/words\"\n",
    "\n",
    "\n",
    "for filename in os.listdir(lemmas_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(lemmas_dir, filename)\n",
    "        df = pd.read_csv(file_path, delimiter=\";\")\n",
    "        updated_df = calculate_simpleMathsScore(df)\n",
    "        updated_df.to_csv(file_path, index=False, sep=\";\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(words_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(words_dir, filename)\n",
    "        df = pd.read_csv(file_path, delimiter=\";\")\n",
    "        updated_df = calculate_simpleMathsScore(df)\n",
    "        updated_df.to_csv(file_path, index=False, sep=\";\")\n",
    "\n",
    "print(\"All files have been updated with simpleMathsScore.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee3e8978-2a4c-4b75-ab1b-f3f87d5dfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT LOWER CASE TRUE AS LEMMA AND WORDS; EXPORT LOWER CASE FALSE AS LEMMA_LCF AND WORDS_LCF\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def ensure_output_dirs(base_dir, suffix):\n",
    "    \"\"\"Create specified output directories for lemmas and words with a given suffix.\"\"\"\n",
    "    lemmas_output_dir = os.path.join(base_dir, \"lemmas\" + suffix)\n",
    "    words_output_dir = os.path.join(base_dir, \"words\" + suffix)\n",
    "    os.makedirs(lemmas_output_dir, exist_ok=True)\n",
    "    os.makedirs(words_output_dir, exist_ok=True)\n",
    "    return lemmas_output_dir, words_output_dir\n",
    "\n",
    "\n",
    "def process_files(directory, output_directory, fc_count_threshold):\n",
    "    \"\"\"Process files to filter data and write to the output directory.\"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath, sep=\";\", encoding=\"utf-8\")\n",
    "            df = df[\n",
    "                (~df[\"upos\"].isin([\"PUNCT\", \"NUM\"]))\n",
    "                & (df[\"fc_count\"] >= fc_count_threshold)\n",
    "                & (df[\"stopword\"] != \"yes\")\n",
    "            ]\n",
    "            cols_to_keep = [\n",
    "                \"lemma\" if \"lemma\" in df.columns else \"word\",\n",
    "                \"fc_count\",\n",
    "                \"stopword\",\n",
    "                \"rfc_count\",\n",
    "                \"fcTotalCount\",\n",
    "                \"rfcTotalCount\",\n",
    "                \"simpleMathsScore\",\n",
    "            ]\n",
    "            df = df[cols_to_keep].sort_values(by=\"simpleMathsScore\", ascending=False)\n",
    "            output_filepath = os.path.join(output_directory, filename)\n",
    "            df.to_csv(output_filepath, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "scores_base_dir = \"scores/SimpleMaths\"\n",
    "\n",
    "\n",
    "lemmas_lcf_dir = \"SimpleMaths/keynessData/lemmas\"\n",
    "words_lcf_dir = \"SimpleMaths/keynessData/words\"\n",
    "scores_lemmas_lcf_dir, scores_words_lcf_dir = ensure_output_dirs(\n",
    "    scores_base_dir, \"_LCF\"\n",
    ")\n",
    "\n",
    "\n",
    "lemmas_lct_dir = \"SimpleMaths/keynessData_LCT/lemmas\"\n",
    "words_lct_dir = \"SimpleMaths/keynessData_LCT/words\"\n",
    "scores_lemmas_lct_dir, scores_words_lct_dir = ensure_output_dirs(scores_base_dir, \"\")\n",
    "\n",
    "\n",
    "fc_count_threshold = 1\n",
    "# Process LCF\n",
    "process_files(lemmas_lcf_dir, scores_lemmas_lcf_dir, fc_count_threshold)\n",
    "process_files(words_lcf_dir, scores_words_lcf_dir, fc_count_threshold)\n",
    "# Process LCT\n",
    "process_files(lemmas_lct_dir, scores_lemmas_lct_dir, fc_count_threshold)\n",
    "process_files(words_lct_dir, scores_words_lct_dir, fc_count_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5746a-b260-43ac-92bd-b8192868d7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
